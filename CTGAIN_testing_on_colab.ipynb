{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw1dW4jzS/SJq5XEBd8a3t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ILTRENTA/CTGAIN/blob/main/CTGAIN_testing_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq8RIHskkRwY",
        "outputId": "2f683ee8-da6f-44b4-8390-6f0808f9060b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CTGAIN'...\n",
            "remote: Enumerating objects: 200, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 200 (delta 33), reused 66 (delta 25), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (200/200), 2.53 MiB | 14.87 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining rdt from git+https://github.com/ILTRENTA/RDT.git@2cf799b7c305430b9932fd90c45283492c8f4aeb#egg=rdt (from -r CTGAIN/requirements_ctgain.txt (line 46))\n",
            "  Cloning https://github.com/ILTRENTA/RDT.git (to revision 2cf799b7c305430b9932fd90c45283492c8f4aeb) to ./src/rdt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ILTRENTA/RDT.git /content/src/rdt\n",
            "  Running command git rev-parse -q --verify 'sha^2cf799b7c305430b9932fd90c45283492c8f4aeb'\n",
            "  Running command git fetch -q https://github.com/ILTRENTA/RDT.git 2cf799b7c305430b9932fd90c45283492c8f4aeb\n",
            "  Resolved https://github.com/ILTRENTA/RDT.git to commit 2cf799b7c305430b9932fd90c45283492c8f4aeb\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting asttokens\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 3)) (2022.12.7)\n",
            "Collecting contourpy==1.0.7\n",
            "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ctgan==0.7.0\n",
            "  Downloading ctgan-0.7.0-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: cycler==0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: debugpy in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 7)) (1.6.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 9)) (0.4)\n",
            "Collecting executing\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting Faker==17.0.0\n",
            "  Downloading Faker-17.0.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flit_core\n",
            "  Downloading flit_core-3.8.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fonttools==4.38.0 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 13)) (4.38.0)\n",
            "Collecting gmpy2\n",
            "  Downloading gmpy2-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 15)) (5.3.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 16)) (7.9.0)\n",
            "Collecting jedi\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 18)) (1.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 19)) (6.1.12)\n",
            "Requirement already satisfied: jupyter_core in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 20)) (5.2.0)\n",
            "Requirement already satisfied: kiwisolver==1.4.4 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 21)) (1.4.4)\n",
            "Collecting matplotlib==3.7.0\n",
            "  Downloading matplotlib-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: mpmath==1.2.1 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 24)) (1.2.1)\n",
            "Collecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting numpy==1.24.2\n",
            "  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 31)) (23.0)\n",
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: parso in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 33)) (0.8.3)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 34)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 35)) (0.7.5)\n",
            "Collecting Pillow==9.4.0\n",
            "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 37)) (2.0.10)\n",
            "Collecting psutil==5.9.4\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 39)) (0.7.0)\n",
            "Collecting pure-eval\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 41)) (2.6.1)\n",
            "Requirement already satisfied: pyparsing==3.0.9 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 42)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 43)) (2.8.2)\n",
            "Requirement already satisfied: pytz==2022.7.1 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 44)) (2022.7.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 45)) (23.2.1)\n",
            "Requirement already satisfied: scikit-learn==1.2.1 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 47)) (1.2.1)\n",
            "Collecting scipy==1.10.0\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seaborn==0.12.2\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 KB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 50)) (1.15.0)\n",
            "Collecting stack-data\n",
            "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 52)) (1.7.1)\n",
            "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 53)) (3.1.0)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 54)) (1.13.1+cu116)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 55)) (6.2)\n",
            "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 56)) (4.64.1)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 57)) (5.7.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 58)) (4.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from -r CTGAIN/requirements_ctgain.txt (line 59)) (0.2.6)\n",
            "Collecting packaging\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7.0->-r CTGAIN/requirements_ctgain.txt (line 22)) (5.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r CTGAIN/requirements_ctgain.txt (line 27)) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r CTGAIN/requirements_ctgain.txt (line 27)) (0.38.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter_core->-r CTGAIN/requirements_ctgain.txt (line 20)) (3.0.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib==3.7.0->-r CTGAIN/requirements_ctgain.txt (line 22)) (3.15.0)\n",
            "Installing collected packages: pure-eval, gmpy2, executing, psutil, Pillow, packaging, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, nest-asyncio, matplotlib-inline, jedi, flit_core, asttokens, stack-data, scipy, pandas, nvidia-cudnn-cu11, Faker, contourpy, matplotlib, seaborn, rdt, ctgan\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.3\n",
            "    Uninstalling matplotlib-3.5.3:\n",
            "      Successfully uninstalled matplotlib-3.5.3\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.11.2\n",
            "    Uninstalling seaborn-0.11.2:\n",
            "      Successfully uninstalled seaborn-0.11.2\n",
            "  Running setup.py develop for rdt\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Faker-17.0.0 Pillow-9.4.0 asttokens-2.2.1 contourpy-1.0.7 ctgan-0.7.0 executing-1.2.0 flit_core-3.8.0 gmpy2-2.1.5 jedi-0.18.2 matplotlib-3.7.0 matplotlib-inline-0.1.6 nest-asyncio-1.5.6 numpy-1.24.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 packaging-21.3 pandas-1.5.3 psutil-5.9.4 pure-eval-0.2.2 rdt-1.3.1.dev0 scipy-1.10.0 seaborn-0.12.2 stack-data-0.6.2\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ILTRENTA/CTGAIN.git\n",
        "\n",
        "!pip install -r CTGAIN/requirements_ctgain.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANT: You must restart your runtime before running the code below, otherwise it won't work"
      ],
      "metadata": {
        "id": "cRNbBgUoGVUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "\n",
        "os.chdir(\"CTGAIN\")\n",
        "import torch \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_wine, load_breast_cancer\n",
        "from ctgain.ctgain_v2 import CTGAIN_v2 as CTGAIN\n",
        "\n",
        "from ctgain.utils.ctgain import *\n",
        "from ctgain.utils.GAIN_metrics import *"
      ],
      "metadata": {
        "id": "h7aCttI6lKL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name=\"abalone.csv\"#@param  [\"abalone.csv\" , \"  diamonds.csv\" , \" hotel.csv\" ,\"red-wine.csv\",\"concrete.csv\" ,\"fuel.csv\" , \"     housing.csv\" , \" Letter.csv\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BHmqGHk2odJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(f\"/content/CTGAIN/datasets/{dataset_name}.csv\")\n",
        "print(data.head())\n",
        "\n",
        "cols=data.columns\n",
        "\n",
        "Mechanism=\"MCAR\"#@param [\"MCAR\", \"MAR\"]\n",
        "\n",
        "# p_miss and p_obs should always add <1 otherwise produce_NA will raise an error\n",
        "# If MCAR is the mechanism, p_obs doesn't affect the missingness \n",
        "trans_dta=produce_NA(data.values, p_miss=.2, \n",
        "                     p_obs=.5, mecha=Mechanism)\n",
        "\n",
        "\n",
        "\n",
        "incomp_dta=trans_dta[\"X_incomp\"]\n",
        "print(incomp_dta.shape)\n",
        "\n",
        "incomp_dta=pd.DataFrame(incomp_dta, columns=cols)\n",
        "incomp_dta.head()\n",
        "\n",
        "\n",
        "model=CTGAIN(epochs=1000, #verbose=True, The current implementation includes a tqdm loading bar which resutls in an ugly output hence keep verbose =False\n",
        "              batch_size=256,\n",
        "             hint_rate=0.9, # hint rate helps the discriminator with guessing the mask matrix\n",
        "             alpha=1)# Alpha in the experiment is set also to 26 \n",
        "\n",
        "\n",
        "model.fit(incomp_dta)\n",
        "display(pd.DataFrame(trans_dta[\"X_init\"],columns=cols))\n",
        "display(incomp_dta)\n",
        "\n",
        "mask=1-incomp_dta.isnull().astype(int).values\n",
        "\n",
        "#model.impute can return can also output the estimates of the observed data\n",
        "# if real_only=True  the model returns the original data + the imputation \n",
        "# if real_only=False then it outputs the generated sample data with the imputation in it\n",
        "#imputed, _, _,=model.impute(incomp_dta.copy(), real_only=False)\n",
        "\n",
        "imputed=model.impute(incomp_dta.copy())\n",
        "display(imputed)\n",
        "print(rmse_loss(data.values,imputed.values ,mask))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDpBvMLKo_Zs",
        "outputId": "dead2ddc-5922-4f4f-8eb9-8b2ec5c2c2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
            "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
            "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
            "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
            "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
            "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
            "\n",
            "   Longitude  MedHouseVal  \n",
            "0    -122.23        4.526  \n",
            "1    -122.22        3.585  \n",
            "2    -122.24        3.521  \n",
            "3    -122.25        3.413  \n",
            "4    -122.25        3.422  \n",
            "torch.Size([20640, 9])\n",
            "2\n",
            "Epoch 1, Loss G:  69.8866, Loss D:  0.4927  g_loss mse comp   2.6859  0.0524\n",
            "Epoch 2, Loss G:  68.5427, Loss D:  0.4851  g_loss mse comp   2.6342  0.0532\n",
            "Epoch 3, Loss G:  66.6349, Loss D:  0.4803  g_loss mse comp   2.5609  0.0511\n",
            "Epoch 4, Loss G:  65.0951, Loss D:  0.4911  g_loss mse comp   2.5018  0.0487\n",
            "Epoch 5, Loss G:  67.5392, Loss D:  0.5117  g_loss mse comp   2.5953  0.0609\n",
            "Epoch 6, Loss G:  65.4801, Loss D:  0.4579  g_loss mse comp   2.5165  0.0515\n",
            "Epoch 7, Loss G:  66.2666, Loss D:  0.4938  g_loss mse comp   2.5467  0.0524\n",
            "Epoch 8, Loss G:  66.7144, Loss D:  0.4771  g_loss mse comp   2.5637  0.0577\n",
            "Epoch 9, Loss G:  66.0594, Loss D:  0.4861  g_loss mse comp   2.5386  0.0550\n",
            "Epoch 10, Loss G:  65.8578, Loss D:  0.4831  g_loss mse comp   2.5311  0.0504\n",
            "Epoch 11, Loss G:  66.0269, Loss D:  0.4773  g_loss mse comp   2.5374  0.0534\n",
            "Epoch 12, Loss G:  67.2851, Loss D:  0.4655  g_loss mse comp   2.5857  0.0578\n",
            "Epoch 13, Loss G:  66.0566, Loss D:  0.4818  g_loss mse comp   2.5386  0.0543\n",
            "Epoch 14, Loss G:  64.0485, Loss D:  0.4771  g_loss mse comp   2.4615  0.0485\n",
            "Epoch 15, Loss G:  65.9253, Loss D:  0.4763  g_loss mse comp   2.5333  0.0595\n",
            "Epoch 16, Loss G:  66.8945, Loss D:  0.4675  g_loss mse comp   2.5702  0.0697\n",
            "Epoch 17, Loss G:  66.1452, Loss D:  0.4543  g_loss mse comp   2.5414  0.0676\n",
            "Epoch 18, Loss G:  66.6303, Loss D:  0.4505  g_loss mse comp   2.5600  0.0696\n",
            "Epoch 19, Loss G:  67.2793, Loss D:  0.4498  g_loss mse comp   2.5847  0.0758\n",
            "Epoch 20, Loss G:  64.8168, Loss D:  0.4157  g_loss mse comp   2.4903  0.0684\n",
            "Epoch 21, Loss G:  65.8152, Loss D:  0.4454  g_loss mse comp   2.5284  0.0768\n",
            "Epoch 22, Loss G:  67.0115, Loss D:  0.4352  g_loss mse comp   2.5743  0.0795\n",
            "Epoch 23, Loss G:  67.6332, Loss D:  0.4103  g_loss mse comp   2.5979  0.0872\n",
            "Epoch 24, Loss G:  66.9943, Loss D:  0.4235  g_loss mse comp   2.5734  0.0850\n",
            "Epoch 25, Loss G:  67.6186, Loss D:  0.4163  g_loss mse comp   2.5974  0.0865\n",
            "Epoch 26, Loss G:  65.1946, Loss D:  0.4240  g_loss mse comp   2.5046  0.0755\n",
            "Epoch 27, Loss G:  66.1503, Loss D:  0.4246  g_loss mse comp   2.5410  0.0854\n",
            "Epoch 28, Loss G:  65.8759, Loss D:  0.3871  g_loss mse comp   2.5306  0.0807\n",
            "Epoch 29, Loss G:  66.9438, Loss D:  0.4013  g_loss mse comp   2.5714  0.0875\n",
            "Epoch 30, Loss G:  66.0439, Loss D:  0.4165  g_loss mse comp   2.5368  0.0873\n",
            "Epoch 31, Loss G:  67.3667, Loss D:  0.4064  g_loss mse comp   2.5875  0.0928\n",
            "Epoch 32, Loss G:  66.3293, Loss D:  0.4222  g_loss mse comp   2.5478  0.0859\n",
            "Epoch 33, Loss G:  66.1758, Loss D:  0.4082  g_loss mse comp   2.5419  0.0872\n",
            "Epoch 34, Loss G:  67.2517, Loss D:  0.4105  g_loss mse comp   2.5831  0.0908\n",
            "Epoch 35, Loss G:  66.9052, Loss D:  0.4147  g_loss mse comp   2.5695  0.0973\n",
            "Epoch 36, Loss G:  66.1578, Loss D:  0.3997  g_loss mse comp   2.5409  0.0949\n",
            "Epoch 37, Loss G:  65.6130, Loss D:  0.3955  g_loss mse comp   2.5201  0.0896\n",
            "Epoch 38, Loss G:  66.7290, Loss D:  0.3942  g_loss mse comp   2.5630  0.0914\n",
            "Epoch 39, Loss G:  66.9908, Loss D:  0.3921  g_loss mse comp   2.5728  0.0980\n",
            "Epoch 40, Loss G:  66.1682, Loss D:  0.3828  g_loss mse comp   2.5414  0.0917\n",
            "Epoch 41, Loss G:  65.7245, Loss D:  0.3912  g_loss mse comp   2.5243  0.0926\n",
            "Epoch 42, Loss G:  67.1974, Loss D:  0.3913  g_loss mse comp   2.5807  0.0993\n",
            "Epoch 43, Loss G:  67.2693, Loss D:  0.3982  g_loss mse comp   2.5834  0.1012\n",
            "Epoch 44, Loss G:  65.8485, Loss D:  0.3715  g_loss mse comp   2.5291  0.0931\n",
            "Epoch 45, Loss G:  65.9592, Loss D:  0.3771  g_loss mse comp   2.5334  0.0912\n",
            "Epoch 46, Loss G:  66.4669, Loss D:  0.3759  g_loss mse comp   2.5526  0.0981\n",
            "Epoch 47, Loss G:  65.6022, Loss D:  0.3770  g_loss mse comp   2.5196  0.0932\n",
            "Epoch 48, Loss G:  66.6323, Loss D:  0.3830  g_loss mse comp   2.5589  0.1004\n",
            "Epoch 49, Loss G:  65.2265, Loss D:  0.3868  g_loss mse comp   2.5052  0.0913\n",
            "Epoch 50, Loss G:  66.9695, Loss D:  0.3689  g_loss mse comp   2.5719  0.1002\n",
            "Epoch 51, Loss G:  66.3232, Loss D:  0.3926  g_loss mse comp   2.5471  0.0978\n",
            "Epoch 52, Loss G:  65.9109, Loss D:  0.3947  g_loss mse comp   2.5315  0.0924\n",
            "Epoch 53, Loss G:  66.0742, Loss D:  0.3665  g_loss mse comp   2.5376  0.0970\n",
            "Epoch 54, Loss G:  65.8921, Loss D:  0.3908  g_loss mse comp   2.5305  0.0990\n",
            "Epoch 55, Loss G:  64.1977, Loss D:  0.3696  g_loss mse comp   2.4658  0.0859\n",
            "Epoch 56, Loss G:  65.4090, Loss D:  0.3880  g_loss mse comp   2.5121  0.0933\n",
            "Epoch 57, Loss G:  65.5251, Loss D:  0.3606  g_loss mse comp   2.5168  0.0893\n",
            "Epoch 58, Loss G:  65.3418, Loss D:  0.3855  g_loss mse comp   2.5097  0.0890\n",
            "Epoch 59, Loss G:  66.9567, Loss D:  0.4002  g_loss mse comp   2.5713  0.1041\n",
            "Epoch 60, Loss G:  65.6024, Loss D:  0.3792  g_loss mse comp   2.5194  0.0969\n",
            "Epoch 61, Loss G:  66.1558, Loss D:  0.3528  g_loss mse comp   2.5408  0.0951\n",
            "Epoch 62, Loss G:  66.6148, Loss D:  0.3929  g_loss mse comp   2.5584  0.0969\n",
            "Epoch 63, Loss G:  64.2878, Loss D:  0.3535  g_loss mse comp   2.4694  0.0840\n",
            "Epoch 64, Loss G:  65.1581, Loss D:  0.3824  g_loss mse comp   2.5026  0.0914\n",
            "Epoch 65, Loss G:  65.6531, Loss D:  0.3620  g_loss mse comp   2.5214  0.0954\n",
            "Epoch 66, Loss G:  66.4590, Loss D:  0.3650  g_loss mse comp   2.5523  0.0995\n",
            "Epoch 67, Loss G:  66.8942, Loss D:  0.3974  g_loss mse comp   2.5689  0.1021\n",
            "Epoch 68, Loss G:  65.1931, Loss D:  0.3645  g_loss mse comp   2.5039  0.0922\n",
            "Epoch 69, Loss G:  64.9703, Loss D:  0.3904  g_loss mse comp   2.4953  0.0933\n",
            "Epoch 70, Loss G:  65.6823, Loss D:  0.3752  g_loss mse comp   2.5225  0.0970\n",
            "Epoch 71, Loss G:  66.1189, Loss D:  0.3797  g_loss mse comp   2.5391  0.1019\n",
            "Epoch 72, Loss G:  65.6128, Loss D:  0.3721  g_loss mse comp   2.5198  0.0968\n",
            "Epoch 73, Loss G:  66.0232, Loss D:  0.3525  g_loss mse comp   2.5356  0.0970\n",
            "Epoch 74, Loss G:  64.9208, Loss D:  0.3649  g_loss mse comp   2.4934  0.0919\n",
            "Epoch 75, Loss G:  65.8456, Loss D:  0.3774  g_loss mse comp   2.5286  0.1021\n",
            "Epoch 76, Loss G:  65.2638, Loss D:  0.3518  g_loss mse comp   2.5065  0.0957\n",
            "Epoch 77, Loss G:  66.1727, Loss D:  0.3678  g_loss mse comp   2.5414  0.0976\n",
            "Epoch 78, Loss G:  66.5439, Loss D:  0.3662  g_loss mse comp   2.5553  0.1067\n",
            "Epoch 79, Loss G:  65.8433, Loss D:  0.3642  g_loss mse comp   2.5286  0.1004\n",
            "Epoch 80, Loss G:  66.3524, Loss D:  0.3612  g_loss mse comp   2.5479  0.1076\n",
            "Epoch 81, Loss G:  65.4365, Loss D:  0.3611  g_loss mse comp   2.5131  0.0965\n",
            "Epoch 82, Loss G:  65.8369, Loss D:  0.3522  g_loss mse comp   2.5282  0.1039\n",
            "Epoch 83, Loss G:  67.3494, Loss D:  0.3442  g_loss mse comp   2.5862  0.1076\n",
            "Epoch 84, Loss G:  65.9368, Loss D:  0.3619  g_loss mse comp   2.5318  0.1094\n",
            "Epoch 85, Loss G:  66.5555, Loss D:  0.3677  g_loss mse comp   2.5557  0.1080\n",
            "Epoch 86, Loss G:  65.9674, Loss D:  0.3566  g_loss mse comp   2.5333  0.1019\n",
            "Epoch 87, Loss G:  66.2779, Loss D:  0.3518  g_loss mse comp   2.5450  0.1078\n",
            "Epoch 88, Loss G:  66.4529, Loss D:  0.3487  g_loss mse comp   2.5518  0.1060\n",
            "Epoch 89, Loss G:  66.2002, Loss D:  0.3661  g_loss mse comp   2.5422  0.1041\n",
            "Epoch 90, Loss G:  67.3250, Loss D:  0.3534  g_loss mse comp   2.5851  0.1116\n",
            "Epoch 91, Loss G:  66.1985, Loss D:  0.3503  g_loss mse comp   2.5420  0.1068\n",
            "Epoch 92, Loss G:  67.6288, Loss D:  0.3512  g_loss mse comp   2.5967  0.1157\n",
            "Epoch 93, Loss G:  65.7565, Loss D:  0.3393  g_loss mse comp   2.5252  0.1024\n",
            "Epoch 94, Loss G:  65.4127, Loss D:  0.3591  g_loss mse comp   2.5119  0.1029\n",
            "Epoch 95, Loss G:  66.7819, Loss D:  0.3434  g_loss mse comp   2.5643  0.1101\n",
            "Epoch 96, Loss G:  66.2361, Loss D:  0.3664  g_loss mse comp   2.5433  0.1103\n",
            "Epoch 97, Loss G:  65.5162, Loss D:  0.3400  g_loss mse comp   2.5157  0.1075\n",
            "Epoch 98, Loss G:  66.9093, Loss D:  0.3484  g_loss mse comp   2.5691  0.1137\n",
            "Epoch 99, Loss G:  66.2176, Loss D:  0.3520  g_loss mse comp   2.5426  0.1090\n",
            "Epoch 100, Loss G:  65.1928, Loss D:  0.3441  g_loss mse comp   2.5036  0.0981\n",
            "Epoch 101, Loss G:  67.4979, Loss D:  0.3695  g_loss mse comp   2.5916  0.1167\n",
            "Epoch 102, Loss G:  66.3494, Loss D:  0.3311  g_loss mse comp   2.5478  0.1062\n",
            "Epoch 103, Loss G:  66.6420, Loss D:  0.3521  g_loss mse comp   2.5590  0.1070\n",
            "Epoch 104, Loss G:  65.5983, Loss D:  0.3414  g_loss mse comp   2.5191  0.1026\n",
            "Epoch 105, Loss G:  65.7578, Loss D:  0.3608  g_loss mse comp   2.5249  0.1098\n",
            "Epoch 106, Loss G:  66.6687, Loss D:  0.3486  g_loss mse comp   2.5597  0.1153\n",
            "Epoch 107, Loss G:  65.8242, Loss D:  0.3611  g_loss mse comp   2.5277  0.1041\n",
            "Epoch 108, Loss G:  66.8241, Loss D:  0.3451  g_loss mse comp   2.5660  0.1072\n",
            "Epoch 109, Loss G:  66.8635, Loss D:  0.3340  g_loss mse comp   2.5673  0.1139\n",
            "Epoch 110, Loss G:  67.1828, Loss D:  0.3318  g_loss mse comp   2.5795  0.1151\n",
            "Epoch 111, Loss G:  66.5389, Loss D:  0.3566  g_loss mse comp   2.5548  0.1144\n",
            "Epoch 112, Loss G:  66.2644, Loss D:  0.3622  g_loss mse comp   2.5444  0.1093\n",
            "Epoch 113, Loss G:  68.9466, Loss D:  0.3395  g_loss mse comp   2.6471  0.1230\n",
            "Epoch 114, Loss G:  66.7808, Loss D:  0.3350  g_loss mse comp   2.5640  0.1172\n",
            "Epoch 115, Loss G:  66.0244, Loss D:  0.3247  g_loss mse comp   2.5354  0.1044\n",
            "Epoch 116, Loss G:  66.0214, Loss D:  0.3337  g_loss mse comp   2.5352  0.1059\n",
            "Epoch 117, Loss G:  64.4259, Loss D:  0.3564  g_loss mse comp   2.4743  0.0930\n",
            "Epoch 118, Loss G:  67.3503, Loss D:  0.3303  g_loss mse comp   2.5859  0.1179\n",
            "Epoch 119, Loss G:  66.3157, Loss D:  0.3283  g_loss mse comp   2.5462  0.1148\n",
            "Epoch 120, Loss G:  65.8476, Loss D:  0.3599  g_loss mse comp   2.5285  0.1075\n",
            "Epoch 121, Loss G:  66.3012, Loss D:  0.3421  g_loss mse comp   2.5457  0.1130\n",
            "Epoch 122, Loss G:  66.9970, Loss D:  0.3456  g_loss mse comp   2.5724  0.1142\n",
            "Epoch 123, Loss G:  66.2188, Loss D:  0.3402  g_loss mse comp   2.5427  0.1087\n",
            "Epoch 124, Loss G:  67.0360, Loss D:  0.3303  g_loss mse comp   2.5740  0.1121\n",
            "Epoch 125, Loss G:  65.0362, Loss D:  0.3446  g_loss mse comp   2.4975  0.1015\n",
            "Epoch 126, Loss G:  65.2337, Loss D:  0.3202  g_loss mse comp   2.5049  0.1064\n",
            "Epoch 127, Loss G:  66.9197, Loss D:  0.3399  g_loss mse comp   2.5695  0.1129\n",
            "Epoch 128, Loss G:  66.9939, Loss D:  0.3456  g_loss mse comp   2.5723  0.1137\n",
            "Epoch 129, Loss G:  65.2726, Loss D:  0.3297  g_loss mse comp   2.5066  0.1018\n",
            "Epoch 130, Loss G:  65.9584, Loss D:  0.3388  g_loss mse comp   2.5324  0.1169\n",
            "Epoch 131, Loss G:  66.7415, Loss D:  0.3320  g_loss mse comp   2.5624  0.1183\n",
            "Epoch 132, Loss G:  65.6363, Loss D:  0.3353  g_loss mse comp   2.5203  0.1092\n",
            "Epoch 133, Loss G:  66.7238, Loss D:  0.3427  g_loss mse comp   2.5619  0.1145\n",
            "Epoch 134, Loss G:  64.6572, Loss D:  0.3343  g_loss mse comp   2.4827  0.1066\n",
            "Epoch 135, Loss G:  65.5042, Loss D:  0.3327  g_loss mse comp   2.5155  0.1015\n",
            "Epoch 136, Loss G:  66.1521, Loss D:  0.3349  g_loss mse comp   2.5401  0.1101\n",
            "Epoch 137, Loss G:  67.2069, Loss D:  0.3301  g_loss mse comp   2.5804  0.1175\n",
            "Epoch 138, Loss G:  65.7617, Loss D:  0.3247  g_loss mse comp   2.5251  0.1097\n",
            "Epoch 139, Loss G:  65.3621, Loss D:  0.3239  g_loss mse comp   2.5098  0.1062\n",
            "Epoch 140, Loss G:  65.4340, Loss D:  0.3369  g_loss mse comp   2.5126  0.1068\n",
            "Epoch 141, Loss G:  66.8789, Loss D:  0.3328  g_loss mse comp   2.5678  0.1165\n",
            "Epoch 142, Loss G:  66.2492, Loss D:  0.3410  g_loss mse comp   2.5437  0.1121\n",
            "Epoch 143, Loss G:  66.7373, Loss D:  0.3277  g_loss mse comp   2.5624  0.1158\n",
            "Epoch 144, Loss G:  65.5373, Loss D:  0.3451  g_loss mse comp   2.5164  0.1103\n",
            "Epoch 145, Loss G:  66.1017, Loss D:  0.3326  g_loss mse comp   2.5382  0.1087\n",
            "Epoch 146, Loss G:  65.6564, Loss D:  0.3449  g_loss mse comp   2.5211  0.1088\n",
            "Epoch 147, Loss G:  66.4102, Loss D:  0.3332  g_loss mse comp   2.5498  0.1141\n",
            "Epoch 148, Loss G:  65.9537, Loss D:  0.3336  g_loss mse comp   2.5322  0.1158\n",
            "Epoch 149, Loss G:  65.8224, Loss D:  0.3179  g_loss mse comp   2.5272  0.1164\n",
            "Epoch 150, Loss G:  65.1459, Loss D:  0.3408  g_loss mse comp   2.5016  0.1053\n",
            "Epoch 151, Loss G:  65.6175, Loss D:  0.3250  g_loss mse comp   2.5195  0.1109\n",
            "Epoch 152, Loss G:  66.2364, Loss D:  0.3367  g_loss mse comp   2.5433  0.1117\n",
            "Epoch 153, Loss G:  66.6660, Loss D:  0.3298  g_loss mse comp   2.5594  0.1207\n",
            "Epoch 154, Loss G:  66.0562, Loss D:  0.3404  g_loss mse comp   2.5364  0.1101\n",
            "Epoch 155, Loss G:  65.2639, Loss D:  0.3353  g_loss mse comp   2.5059  0.1114\n",
            "Epoch 156, Loss G:  64.8934, Loss D:  0.3253  g_loss mse comp   2.4918  0.1070\n",
            "Epoch 157, Loss G:  65.8822, Loss D:  0.3395  g_loss mse comp   2.5298  0.1082\n",
            "Epoch 158, Loss G:  65.2438, Loss D:  0.3193  g_loss mse comp   2.5054  0.1043\n",
            "Epoch 159, Loss G:  65.3761, Loss D:  0.3317  g_loss mse comp   2.5104  0.1069\n",
            "Epoch 160, Loss G:  66.1303, Loss D:  0.3396  g_loss mse comp   2.5390  0.1162\n",
            "Epoch 161, Loss G:  67.3656, Loss D:  0.3255  g_loss mse comp   2.5863  0.1222\n",
            "Epoch 162, Loss G:  66.9339, Loss D:  0.3331  g_loss mse comp   2.5699  0.1172\n",
            "Epoch 163, Loss G:  65.8148, Loss D:  0.3355  g_loss mse comp   2.5267  0.1193\n",
            "Epoch 164, Loss G:  65.9364, Loss D:  0.3417  g_loss mse comp   2.5316  0.1153\n",
            "Epoch 165, Loss G:  66.6869, Loss D:  0.3329  g_loss mse comp   2.5604  0.1160\n",
            "Epoch 166, Loss G:  66.4415, Loss D:  0.3279  g_loss mse comp   2.5509  0.1178\n",
            "Epoch 167, Loss G:  65.8924, Loss D:  0.3298  g_loss mse comp   2.5299  0.1149\n",
            "Epoch 168, Loss G:  66.7952, Loss D:  0.3090  g_loss mse comp   2.5644  0.1200\n",
            "Epoch 169, Loss G:  66.1045, Loss D:  0.3336  g_loss mse comp   2.5381  0.1144\n",
            "Epoch 170, Loss G:  67.4665, Loss D:  0.3170  g_loss mse comp   2.5900  0.1254\n",
            "Epoch 171, Loss G:  66.2685, Loss D:  0.3238  g_loss mse comp   2.5443  0.1159\n",
            "Epoch 172, Loss G:  65.9500, Loss D:  0.3241  g_loss mse comp   2.5323  0.1114\n",
            "Epoch 173, Loss G:  65.8366, Loss D:  0.3285  g_loss mse comp   2.5278  0.1136\n",
            "Epoch 174, Loss G:  66.3351, Loss D:  0.3274  g_loss mse comp   2.5468  0.1182\n",
            "Epoch 175, Loss G:  67.9685, Loss D:  0.3279  g_loss mse comp   2.6093  0.1260\n",
            "Epoch 176, Loss G:  66.4935, Loss D:  0.3098  g_loss mse comp   2.5528  0.1205\n",
            "Epoch 177, Loss G:  66.5967, Loss D:  0.3350  g_loss mse comp   2.5569  0.1176\n",
            "Epoch 178, Loss G:  66.3590, Loss D:  0.3130  g_loss mse comp   2.5476  0.1205\n",
            "Epoch 179, Loss G:  66.9392, Loss D:  0.3187  g_loss mse comp   2.5700  0.1203\n",
            "Epoch 180, Loss G:  66.3496, Loss D:  0.3089  g_loss mse comp   2.5472  0.1220\n",
            "Epoch 181, Loss G:  66.6424, Loss D:  0.3155  g_loss mse comp   2.5586  0.1185\n",
            "Epoch 182, Loss G:  65.6987, Loss D:  0.3065  g_loss mse comp   2.5226  0.1124\n",
            "Epoch 183, Loss G:  65.6532, Loss D:  0.3173  g_loss mse comp   2.5207  0.1146\n",
            "Epoch 184, Loss G:  66.5512, Loss D:  0.3368  g_loss mse comp   2.5551  0.1197\n",
            "Epoch 185, Loss G:  66.1642, Loss D:  0.3246  g_loss mse comp   2.5404  0.1138\n",
            "Epoch 186, Loss G:  65.4484, Loss D:  0.3135  g_loss mse comp   2.5128  0.1165\n",
            "Epoch 187, Loss G:  65.0497, Loss D:  0.2939  g_loss mse comp   2.4977  0.1096\n",
            "Epoch 188, Loss G:  65.6704, Loss D:  0.3286  g_loss mse comp   2.5214  0.1130\n",
            "Epoch 189, Loss G:  66.5163, Loss D:  0.3214  g_loss mse comp   2.5536  0.1238\n",
            "Epoch 190, Loss G:  66.7752, Loss D:  0.3128  g_loss mse comp   2.5638  0.1170\n",
            "Epoch 191, Loss G:  65.9391, Loss D:  0.3342  g_loss mse comp   2.5316  0.1173\n",
            "Epoch 192, Loss G:  65.3876, Loss D:  0.3131  g_loss mse comp   2.5107  0.1101\n",
            "Epoch 193, Loss G:  65.8215, Loss D:  0.3257  g_loss mse comp   2.5271  0.1161\n",
            "Epoch 194, Loss G:  66.2536, Loss D:  0.3370  g_loss mse comp   2.5436  0.1211\n",
            "Epoch 195, Loss G:  65.7656, Loss D:  0.3185  g_loss mse comp   2.5250  0.1156\n",
            "Epoch 196, Loss G:  65.6723, Loss D:  0.3287  g_loss mse comp   2.5215  0.1136\n",
            "Epoch 197, Loss G:  66.0698, Loss D:  0.3169  g_loss mse comp   2.5366  0.1171\n",
            "Epoch 198, Loss G:  66.2843, Loss D:  0.3167  g_loss mse comp   2.5448  0.1191\n",
            "Epoch 199, Loss G:  65.7441, Loss D:  0.3261  g_loss mse comp   2.5242  0.1152\n",
            "Epoch 200, Loss G:  65.9751, Loss D:  0.3266  g_loss mse comp   2.5331  0.1147\n",
            "Epoch 201, Loss G:  66.3847, Loss D:  0.3172  g_loss mse comp   2.5483  0.1280\n",
            "Epoch 202, Loss G:  65.2266, Loss D:  0.3180  g_loss mse comp   2.5045  0.1097\n",
            "Epoch 203, Loss G:  66.8971, Loss D:  0.3345  g_loss mse comp   2.5684  0.1188\n",
            "Epoch 204, Loss G:  65.4604, Loss D:  0.3183  g_loss mse comp   2.5133  0.1158\n",
            "Epoch 205, Loss G:  66.2238, Loss D:  0.3323  g_loss mse comp   2.5425  0.1176\n",
            "Epoch 206, Loss G:  66.8076, Loss D:  0.3259  g_loss mse comp   2.5648  0.1235\n",
            "Epoch 207, Loss G:  66.3341, Loss D:  0.3259  g_loss mse comp   2.5468  0.1183\n",
            "Epoch 208, Loss G:  66.8283, Loss D:  0.3240  g_loss mse comp   2.5656  0.1224\n",
            "Epoch 209, Loss G:  65.7210, Loss D:  0.3197  g_loss mse comp   2.5232  0.1180\n",
            "Epoch 210, Loss G:  65.5555, Loss D:  0.3249  g_loss mse comp   2.5171  0.1097\n",
            "Epoch 211, Loss G:  66.3734, Loss D:  0.3302  g_loss mse comp   2.5483  0.1163\n",
            "Epoch 212, Loss G:  66.7300, Loss D:  0.3225  g_loss mse comp   2.5620  0.1189\n",
            "Epoch 213, Loss G:  66.8406, Loss D:  0.3226  g_loss mse comp   2.5659  0.1270\n",
            "Epoch 214, Loss G:  66.1120, Loss D:  0.3248  g_loss mse comp   2.5383  0.1152\n",
            "Epoch 215, Loss G:  68.4754, Loss D:  0.3133  g_loss mse comp   2.6288  0.1268\n",
            "Epoch 216, Loss G:  66.5650, Loss D:  0.3100  g_loss mse comp   2.5558  0.1152\n",
            "Epoch 217, Loss G:  65.2966, Loss D:  0.2957  g_loss mse comp   2.5071  0.1124\n",
            "Epoch 218, Loss G:  66.2220, Loss D:  0.3337  g_loss mse comp   2.5424  0.1196\n",
            "Epoch 219, Loss G:  66.2986, Loss D:  0.3308  g_loss mse comp   2.5454  0.1192\n",
            "Epoch 220, Loss G:  65.5848, Loss D:  0.3266  g_loss mse comp   2.5181  0.1140\n",
            "Epoch 221, Loss G:  67.6910, Loss D:  0.3329  g_loss mse comp   2.5986  0.1284\n",
            "Epoch 222, Loss G:  65.8229, Loss D:  0.3182  g_loss mse comp   2.5272  0.1167\n",
            "Epoch 223, Loss G:  67.7114, Loss D:  0.3213  g_loss mse comp   2.5995  0.1254\n",
            "Epoch 224, Loss G:  66.0114, Loss D:  0.3248  g_loss mse comp   2.5346  0.1122\n",
            "Epoch 225, Loss G:  66.9463, Loss D:  0.3047  g_loss mse comp   2.5702  0.1211\n",
            "Epoch 226, Loss G:  66.5830, Loss D:  0.3064  g_loss mse comp   2.5563  0.1203\n",
            "Epoch 227, Loss G:  67.3701, Loss D:  0.3141  g_loss mse comp   2.5862  0.1280\n",
            "Epoch 228, Loss G:  65.9921, Loss D:  0.3141  g_loss mse comp   2.5338  0.1144\n",
            "Epoch 229, Loss G:  67.0085, Loss D:  0.3281  g_loss mse comp   2.5726  0.1203\n",
            "Epoch 230, Loss G:  66.1729, Loss D:  0.3314  g_loss mse comp   2.5407  0.1154\n",
            "Epoch 231, Loss G:  67.3376, Loss D:  0.3163  g_loss mse comp   2.5850  0.1287\n",
            "Epoch 232, Loss G:  65.2945, Loss D:  0.3147  g_loss mse comp   2.5068  0.1174\n",
            "Epoch 233, Loss G:  65.7857, Loss D:  0.3164  g_loss mse comp   2.5258  0.1156\n",
            "Epoch 234, Loss G:  65.0780, Loss D:  0.3055  g_loss mse comp   2.4987  0.1116\n",
            "Epoch 235, Loss G:  64.9960, Loss D:  0.3145  g_loss mse comp   2.4956  0.1114\n",
            "Epoch 236, Loss G:  66.7416, Loss D:  0.3325  g_loss mse comp   2.5623  0.1226\n",
            "Epoch 237, Loss G:  65.5055, Loss D:  0.3100  g_loss mse comp   2.5152  0.1107\n",
            "Epoch 238, Loss G:  65.3139, Loss D:  0.3190  g_loss mse comp   2.5079  0.1089\n",
            "Epoch 239, Loss G:  66.8451, Loss D:  0.3215  g_loss mse comp   2.5664  0.1199\n",
            "Epoch 240, Loss G:  65.4247, Loss D:  0.3244  g_loss mse comp   2.5120  0.1138\n",
            "Epoch 241, Loss G:  65.2602, Loss D:  0.3007  g_loss mse comp   2.5056  0.1142\n",
            "Epoch 242, Loss G:  65.2121, Loss D:  0.3093  g_loss mse comp   2.5038  0.1139\n",
            "Epoch 243, Loss G:  66.3160, Loss D:  0.3116  g_loss mse comp   2.5460  0.1193\n",
            "Epoch 244, Loss G:  65.1691, Loss D:  0.3105  g_loss mse comp   2.5022  0.1109\n",
            "Epoch 245, Loss G:  66.4766, Loss D:  0.3263  g_loss mse comp   2.5522  0.1204\n",
            "Epoch 246, Loss G:  67.1737, Loss D:  0.3223  g_loss mse comp   2.5786  0.1295\n",
            "Epoch 247, Loss G:  65.8620, Loss D:  0.3057  g_loss mse comp   2.5287  0.1161\n",
            "Epoch 248, Loss G:  66.7586, Loss D:  0.3049  g_loss mse comp   2.5631  0.1188\n",
            "Epoch 249, Loss G:  65.9267, Loss D:  0.3232  g_loss mse comp   2.5312  0.1158\n",
            "Epoch 250, Loss G:  66.3038, Loss D:  0.3285  g_loss mse comp   2.5456  0.1187\n",
            "Epoch 251, Loss G:  65.6419, Loss D:  0.3340  g_loss mse comp   2.5205  0.1096\n",
            "Epoch 252, Loss G:  65.5078, Loss D:  0.3199  g_loss mse comp   2.5151  0.1147\n",
            "Epoch 253, Loss G:  65.0063, Loss D:  0.3186  g_loss mse comp   2.4959  0.1117\n",
            "Epoch 254, Loss G:  67.1974, Loss D:  0.3151  g_loss mse comp   2.5795  0.1293\n",
            "Epoch 255, Loss G:  66.1893, Loss D:  0.3056  g_loss mse comp   2.5412  0.1190\n",
            "Epoch 256, Loss G:  66.0835, Loss D:  0.3178  g_loss mse comp   2.5372  0.1171\n",
            "Epoch 257, Loss G:  65.9457, Loss D:  0.3075  g_loss mse comp   2.5318  0.1194\n",
            "Epoch 258, Loss G:  65.2297, Loss D:  0.3086  g_loss mse comp   2.5046  0.1100\n",
            "Epoch 259, Loss G:  66.1543, Loss D:  0.3265  g_loss mse comp   2.5399  0.1170\n",
            "Epoch 260, Loss G:  65.4347, Loss D:  0.3009  g_loss mse comp   2.5123  0.1141\n",
            "Epoch 261, Loss G:  65.2892, Loss D:  0.3139  g_loss mse comp   2.5068  0.1115\n",
            "Epoch 262, Loss G:  66.6637, Loss D:  0.3093  g_loss mse comp   2.5594  0.1204\n",
            "Epoch 263, Loss G:  66.8199, Loss D:  0.3128  g_loss mse comp   2.5653  0.1225\n",
            "Epoch 264, Loss G:  67.0731, Loss D:  0.3129  g_loss mse comp   2.5752  0.1181\n",
            "Epoch 265, Loss G:  67.5692, Loss D:  0.3170  g_loss mse comp   2.5938  0.1296\n",
            "Epoch 266, Loss G:  65.4761, Loss D:  0.3314  g_loss mse comp   2.5138  0.1162\n",
            "Epoch 267, Loss G:  65.2855, Loss D:  0.3161  g_loss mse comp   2.5066  0.1142\n",
            "Epoch 268, Loss G:  67.8469, Loss D:  0.3167  g_loss mse comp   2.6047  0.1245\n",
            "Epoch 269, Loss G:  66.0143, Loss D:  0.3075  g_loss mse comp   2.5345  0.1176\n",
            "Epoch 270, Loss G:  64.8636, Loss D:  0.2994  g_loss mse comp   2.4906  0.1085\n",
            "Epoch 271, Loss G:  65.6253, Loss D:  0.3155  g_loss mse comp   2.5196  0.1165\n",
            "Epoch 272, Loss G:  65.3071, Loss D:  0.3196  g_loss mse comp   2.5073  0.1162\n",
            "Epoch 273, Loss G:  65.4839, Loss D:  0.3238  g_loss mse comp   2.5144  0.1095\n",
            "Epoch 274, Loss G:  66.1969, Loss D:  0.3266  g_loss mse comp   2.5416  0.1154\n",
            "Epoch 275, Loss G:  65.9912, Loss D:  0.3237  g_loss mse comp   2.5336  0.1174\n",
            "Epoch 276, Loss G:  66.2839, Loss D:  0.3066  g_loss mse comp   2.5447  0.1207\n",
            "Epoch 277, Loss G:  66.6456, Loss D:  0.3292  g_loss mse comp   2.5587  0.1188\n",
            "Epoch 278, Loss G:  67.7291, Loss D:  0.3234  g_loss mse comp   2.6000  0.1286\n",
            "Epoch 279, Loss G:  66.4312, Loss D:  0.3291  g_loss mse comp   2.5502  0.1248\n",
            "Epoch 280, Loss G:  67.0469, Loss D:  0.3097  g_loss mse comp   2.5741  0.1212\n",
            "Epoch 281, Loss G:  66.5557, Loss D:  0.2974  g_loss mse comp   2.5552  0.1213\n",
            "Epoch 282, Loss G:  67.0445, Loss D:  0.3085  g_loss mse comp   2.5739  0.1240\n",
            "Epoch 283, Loss G:  66.1868, Loss D:  0.3301  g_loss mse comp   2.5411  0.1178\n",
            "Epoch 284, Loss G:  66.8647, Loss D:  0.3135  g_loss mse comp   2.5670  0.1228\n",
            "Epoch 285, Loss G:  66.1523, Loss D:  0.3167  g_loss mse comp   2.5396  0.1220\n",
            "Epoch 286, Loss G:  68.1539, Loss D:  0.3101  g_loss mse comp   2.6163  0.1297\n",
            "Epoch 287, Loss G:  65.9666, Loss D:  0.3258  g_loss mse comp   2.5327  0.1158\n",
            "Epoch 288, Loss G:  65.1719, Loss D:  0.3212  g_loss mse comp   2.5022  0.1156\n",
            "Epoch 289, Loss G:  66.2050, Loss D:  0.3474  g_loss mse comp   2.5419  0.1152\n",
            "Epoch 290, Loss G:  66.3560, Loss D:  0.3076  g_loss mse comp   2.5477  0.1159\n",
            "Epoch 291, Loss G:  65.4604, Loss D:  0.3105  g_loss mse comp   2.5133  0.1158\n",
            "Epoch 292, Loss G:  64.8143, Loss D:  0.3047  g_loss mse comp   2.4887  0.1079\n",
            "Epoch 293, Loss G:  65.8263, Loss D:  0.3146  g_loss mse comp   2.5273  0.1154\n",
            "Epoch 294, Loss G:  65.9202, Loss D:  0.3219  g_loss mse comp   2.5309  0.1172\n",
            "Epoch 295, Loss G:  65.9668, Loss D:  0.3115  g_loss mse comp   2.5326  0.1185\n",
            "Epoch 296, Loss G:  67.2275, Loss D:  0.3220  g_loss mse comp   2.5808  0.1267\n",
            "Epoch 297, Loss G:  66.2707, Loss D:  0.3165  g_loss mse comp   2.5444  0.1173\n",
            "Epoch 298, Loss G:  65.4115, Loss D:  0.3197  g_loss mse comp   2.5114  0.1150\n",
            "Epoch 299, Loss G:  66.2933, Loss D:  0.3084  g_loss mse comp   2.5453  0.1145\n",
            "Epoch 300, Loss G:  66.8379, Loss D:  0.3099  g_loss mse comp   2.5658  0.1264\n",
            "Epoch 301, Loss G:  64.7214, Loss D:  0.3272  g_loss mse comp   2.4851  0.1077\n",
            "Epoch 302, Loss G:  66.1512, Loss D:  0.3056  g_loss mse comp   2.5398  0.1168\n",
            "Epoch 303, Loss G:  67.2665, Loss D:  0.3137  g_loss mse comp   2.5821  0.1315\n",
            "Epoch 304, Loss G:  66.6679, Loss D:  0.3027  g_loss mse comp   2.5595  0.1201\n",
            "Epoch 305, Loss G:  66.3101, Loss D:  0.3129  g_loss mse comp   2.5459  0.1178\n",
            "Epoch 306, Loss G:  66.6901, Loss D:  0.3240  g_loss mse comp   2.5602  0.1261\n",
            "Epoch 307, Loss G:  65.5998, Loss D:  0.3143  g_loss mse comp   2.5186  0.1163\n",
            "Epoch 308, Loss G:  66.9263, Loss D:  0.3319  g_loss mse comp   2.5694  0.1209\n",
            "Epoch 309, Loss G:  65.7157, Loss D:  0.3141  g_loss mse comp   2.5231  0.1151\n",
            "Epoch 310, Loss G:  66.2725, Loss D:  0.3118  g_loss mse comp   2.5442  0.1235\n",
            "Epoch 311, Loss G:  66.2017, Loss D:  0.3407  g_loss mse comp   2.5416  0.1193\n",
            "Epoch 312, Loss G:  65.6730, Loss D:  0.3207  g_loss mse comp   2.5214  0.1172\n",
            "Epoch 313, Loss G:  65.0034, Loss D:  0.3067  g_loss mse comp   2.4958  0.1126\n",
            "Epoch 314, Loss G:  66.4467, Loss D:  0.3336  g_loss mse comp   2.5511  0.1183\n",
            "Epoch 315, Loss G:  67.0128, Loss D:  0.3258  g_loss mse comp   2.5727  0.1218\n",
            "Epoch 316, Loss G:  66.2485, Loss D:  0.3119  g_loss mse comp   2.5435  0.1184\n",
            "Epoch 317, Loss G:  66.2864, Loss D:  0.3165  g_loss mse comp   2.5448  0.1206\n",
            "Epoch 318, Loss G:  63.9482, Loss D:  0.3116  g_loss mse comp   2.4555  0.1048\n",
            "Epoch 319, Loss G:  66.8754, Loss D:  0.3114  g_loss mse comp   2.5674  0.1231\n",
            "Epoch 320, Loss G:  68.0834, Loss D:  0.3070  g_loss mse comp   2.6133  0.1367\n",
            "Epoch 321, Loss G:  66.1536, Loss D:  0.3131  g_loss mse comp   2.5398  0.1177\n",
            "Epoch 322, Loss G:  67.0060, Loss D:  0.3152  g_loss mse comp   2.5725  0.1220\n",
            "Epoch 323, Loss G:  66.4708, Loss D:  0.3204  g_loss mse comp   2.5520  0.1192\n",
            "Epoch 324, Loss G:  67.8068, Loss D:  0.3234  g_loss mse comp   2.6031  0.1261\n",
            "Epoch 325, Loss G:  67.3897, Loss D:  0.3179  g_loss mse comp   2.5871  0.1263\n",
            "Epoch 326, Loss G:  65.8228, Loss D:  0.3153  g_loss mse comp   2.5273  0.1128\n",
            "Epoch 327, Loss G:  66.0472, Loss D:  0.3122  g_loss mse comp   2.5358  0.1166\n",
            "Epoch 328, Loss G:  65.2034, Loss D:  0.3097  g_loss mse comp   2.5036  0.1099\n",
            "Epoch 329, Loss G:  65.0138, Loss D:  0.3193  g_loss mse comp   2.4963  0.1095\n",
            "Epoch 330, Loss G:  66.4837, Loss D:  0.3252  g_loss mse comp   2.5524  0.1203\n",
            "Epoch 331, Loss G:  66.6095, Loss D:  0.3282  g_loss mse comp   2.5572  0.1234\n",
            "Epoch 332, Loss G:  65.7282, Loss D:  0.3050  g_loss mse comp   2.5235  0.1160\n",
            "Epoch 333, Loss G:  65.6543, Loss D:  0.3173  g_loss mse comp   2.5208  0.1141\n",
            "Epoch 334, Loss G:  65.9750, Loss D:  0.3146  g_loss mse comp   2.5329  0.1184\n",
            "Epoch 335, Loss G:  66.2335, Loss D:  0.3037  g_loss mse comp   2.5431  0.1139\n",
            "Epoch 336, Loss G:  65.9911, Loss D:  0.3129  g_loss mse comp   2.5337  0.1144\n",
            "Epoch 337, Loss G:  65.9252, Loss D:  0.3240  g_loss mse comp   2.5312  0.1145\n",
            "Epoch 338, Loss G:  66.9496, Loss D:  0.3088  g_loss mse comp   2.5704  0.1200\n",
            "Epoch 339, Loss G:  65.4823, Loss D:  0.3068  g_loss mse comp   2.5143  0.1108\n",
            "Epoch 340, Loss G:  65.2408, Loss D:  0.3194  g_loss mse comp   2.5049  0.1127\n",
            "Epoch 341, Loss G:  66.5861, Loss D:  0.3202  g_loss mse comp   2.5565  0.1163\n",
            "Epoch 342, Loss G:  65.5494, Loss D:  0.3270  g_loss mse comp   2.5168  0.1134\n",
            "Epoch 343, Loss G:  67.7061, Loss D:  0.3290  g_loss mse comp   2.5992  0.1261\n",
            "Epoch 344, Loss G:  66.5757, Loss D:  0.3213  g_loss mse comp   2.5561  0.1181\n",
            "Epoch 345, Loss G:  66.7223, Loss D:  0.3206  g_loss mse comp   2.5616  0.1200\n",
            "Epoch 346, Loss G:  66.8632, Loss D:  0.3223  g_loss mse comp   2.5669  0.1227\n",
            "Epoch 347, Loss G:  67.1048, Loss D:  0.3233  g_loss mse comp   2.5761  0.1251\n",
            "Epoch 348, Loss G:  66.5398, Loss D:  0.3170  g_loss mse comp   2.5547  0.1189\n",
            "Epoch 349, Loss G:  65.4795, Loss D:  0.3104  g_loss mse comp   2.5142  0.1107\n",
            "Epoch 350, Loss G:  66.4941, Loss D:  0.3184  g_loss mse comp   2.5530  0.1171\n",
            "Epoch 351, Loss G:  66.1813, Loss D:  0.3224  g_loss mse comp   2.5408  0.1208\n",
            "Epoch 352, Loss G:  66.7095, Loss D:  0.3077  g_loss mse comp   2.5610  0.1230\n",
            "Epoch 353, Loss G:  65.7266, Loss D:  0.3217  g_loss mse comp   2.5234  0.1178\n",
            "Epoch 354, Loss G:  67.7337, Loss D:  0.3294  g_loss mse comp   2.6003  0.1262\n",
            "Epoch 355, Loss G:  65.5537, Loss D:  0.3250  g_loss mse comp   2.5169  0.1134\n",
            "Epoch 356, Loss G:  64.8057, Loss D:  0.3222  g_loss mse comp   2.4883  0.1097\n",
            "Epoch 357, Loss G:  66.3715, Loss D:  0.3153  g_loss mse comp   2.5481  0.1212\n",
            "Epoch 358, Loss G:  66.6401, Loss D:  0.3244  g_loss mse comp   2.5586  0.1165\n",
            "Epoch 359, Loss G:  65.7869, Loss D:  0.3207  g_loss mse comp   2.5260  0.1109\n",
            "Epoch 360, Loss G:  65.9331, Loss D:  0.3187  g_loss mse comp   2.5314  0.1167\n",
            "Epoch 361, Loss G:  65.1252, Loss D:  0.3247  g_loss mse comp   2.5005  0.1128\n",
            "Epoch 362, Loss G:  67.1365, Loss D:  0.3208  g_loss mse comp   2.5775  0.1217\n",
            "Epoch 363, Loss G:  67.7361, Loss D:  0.3105  g_loss mse comp   2.6002  0.1307\n",
            "Epoch 364, Loss G:  66.2573, Loss D:  0.3318  g_loss mse comp   2.5438  0.1185\n",
            "Epoch 365, Loss G:  65.5432, Loss D:  0.3092  g_loss mse comp   2.5168  0.1069\n",
            "Epoch 366, Loss G:  66.7582, Loss D:  0.3251  g_loss mse comp   2.5629  0.1225\n",
            "Epoch 367, Loss G:  66.0426, Loss D:  0.3168  g_loss mse comp   2.5356  0.1172\n",
            "Epoch 368, Loss G:  64.8795, Loss D:  0.3182  g_loss mse comp   2.4910  0.1130\n",
            "Epoch 369, Loss G:  66.1068, Loss D:  0.3207  g_loss mse comp   2.5380  0.1187\n",
            "Epoch 370, Loss G:  67.9037, Loss D:  0.3197  g_loss mse comp   2.6067  0.1297\n",
            "Epoch 371, Loss G:  66.6157, Loss D:  0.3308  g_loss mse comp   2.5576  0.1173\n",
            "Epoch 372, Loss G:  66.0536, Loss D:  0.3179  g_loss mse comp   2.5361  0.1147\n",
            "Epoch 373, Loss G:  68.0162, Loss D:  0.3075  g_loss mse comp   2.6114  0.1210\n",
            "Epoch 374, Loss G:  66.2690, Loss D:  0.3268  g_loss mse comp   2.5443  0.1184\n",
            "Epoch 375, Loss G:  65.9277, Loss D:  0.3174  g_loss mse comp   2.5312  0.1161\n",
            "Epoch 376, Loss G:  66.3296, Loss D:  0.3189  g_loss mse comp   2.5465  0.1214\n",
            "Epoch 377, Loss G:  67.1403, Loss D:  0.3314  g_loss mse comp   2.5775  0.1259\n",
            "Epoch 378, Loss G:  66.1758, Loss D:  0.3152  g_loss mse comp   2.5409  0.1125\n",
            "Epoch 379, Loss G:  65.4086, Loss D:  0.3132  g_loss mse comp   2.5114  0.1123\n",
            "Epoch 380, Loss G:  66.9216, Loss D:  0.3238  g_loss mse comp   2.5693  0.1209\n",
            "Epoch 381, Loss G:  65.7008, Loss D:  0.3038  g_loss mse comp   2.5225  0.1164\n",
            "Epoch 382, Loss G:  65.5584, Loss D:  0.3129  g_loss mse comp   2.5171  0.1141\n",
            "Epoch 383, Loss G:  66.9574, Loss D:  0.3184  g_loss mse comp   2.5705  0.1238\n",
            "Epoch 384, Loss G:  66.5838, Loss D:  0.2996  g_loss mse comp   2.5563  0.1191\n",
            "Epoch 385, Loss G:  66.1577, Loss D:  0.3228  g_loss mse comp   2.5400  0.1186\n",
            "Epoch 386, Loss G:  65.7722, Loss D:  0.2983  g_loss mse comp   2.5251  0.1190\n",
            "Epoch 387, Loss G:  66.4389, Loss D:  0.3277  g_loss mse comp   2.5508  0.1175\n",
            "Epoch 388, Loss G:  66.1323, Loss D:  0.3150  g_loss mse comp   2.5390  0.1174\n",
            "Epoch 389, Loss G:  66.2652, Loss D:  0.3166  g_loss mse comp   2.5441  0.1184\n",
            "Epoch 390, Loss G:  66.3363, Loss D:  0.3300  g_loss mse comp   2.5466  0.1245\n",
            "Epoch 391, Loss G:  66.3266, Loss D:  0.3194  g_loss mse comp   2.5466  0.1160\n",
            "Epoch 392, Loss G:  65.9522, Loss D:  0.3174  g_loss mse comp   2.5322  0.1155\n",
            "Epoch 393, Loss G:  65.7682, Loss D:  0.3084  g_loss mse comp   2.5253  0.1115\n",
            "Epoch 394, Loss G:  65.2223, Loss D:  0.3206  g_loss mse comp   2.5042  0.1138\n",
            "Epoch 395, Loss G:  65.2379, Loss D:  0.3171  g_loss mse comp   2.5050  0.1073\n",
            "Epoch 396, Loss G:  67.3703, Loss D:  0.3210  g_loss mse comp   2.5865  0.1211\n",
            "Epoch 397, Loss G:  66.9757, Loss D:  0.3276  g_loss mse comp   2.5714  0.1198\n",
            "Epoch 398, Loss G:  66.7692, Loss D:  0.3239  g_loss mse comp   2.5636  0.1156\n",
            "Epoch 399, Loss G:  65.6620, Loss D:  0.3114  g_loss mse comp   2.5209  0.1190\n",
            "Epoch 400, Loss G:  66.6881, Loss D:  0.3103  g_loss mse comp   2.5606  0.1136\n",
            "Epoch 401, Loss G:  65.7626, Loss D:  0.3260  g_loss mse comp   2.5248  0.1179\n",
            "Epoch 402, Loss G:  65.7635, Loss D:  0.3083  g_loss mse comp   2.5251  0.1121\n",
            "Epoch 403, Loss G:  66.8752, Loss D:  0.3246  g_loss mse comp   2.5673  0.1263\n",
            "Epoch 404, Loss G:  66.7562, Loss D:  0.3147  g_loss mse comp   2.5629  0.1221\n",
            "Epoch 405, Loss G:  67.1041, Loss D:  0.3186  g_loss mse comp   2.5764  0.1187\n",
            "Epoch 406, Loss G:  66.6014, Loss D:  0.3060  g_loss mse comp   2.5571  0.1166\n",
            "Epoch 407, Loss G:  66.1984, Loss D:  0.3236  g_loss mse comp   2.5414  0.1211\n",
            "Epoch 408, Loss G:  65.9951, Loss D:  0.3238  g_loss mse comp   2.5337  0.1179\n",
            "Epoch 409, Loss G:  66.2025, Loss D:  0.3158  g_loss mse comp   2.5418  0.1164\n",
            "Epoch 410, Loss G:  67.2340, Loss D:  0.3157  g_loss mse comp   2.5812  0.1225\n",
            "Epoch 411, Loss G:  65.5907, Loss D:  0.3213  g_loss mse comp   2.5182  0.1168\n",
            "Epoch 412, Loss G:  65.8251, Loss D:  0.3069  g_loss mse comp   2.5272  0.1171\n",
            "Epoch 413, Loss G:  64.5757, Loss D:  0.3259  g_loss mse comp   2.4797  0.1042\n",
            "Epoch 414, Loss G:  66.7991, Loss D:  0.3132  g_loss mse comp   2.5646  0.1183\n",
            "Epoch 415, Loss G:  65.5904, Loss D:  0.3215  g_loss mse comp   2.5183  0.1148\n",
            "Epoch 416, Loss G:  65.7698, Loss D:  0.3308  g_loss mse comp   2.5250  0.1211\n",
            "Epoch 417, Loss G:  66.1107, Loss D:  0.3191  g_loss mse comp   2.5382  0.1171\n",
            "Epoch 418, Loss G:  65.6563, Loss D:  0.3349  g_loss mse comp   2.5209  0.1131\n",
            "Epoch 419, Loss G:  64.6565, Loss D:  0.3103  g_loss mse comp   2.4828  0.1033\n",
            "Epoch 420, Loss G:  65.5278, Loss D:  0.3313  g_loss mse comp   2.5158  0.1182\n",
            "Epoch 421, Loss G:  66.3768, Loss D:  0.3199  g_loss mse comp   2.5484  0.1174\n",
            "Epoch 422, Loss G:  66.1884, Loss D:  0.3247  g_loss mse comp   2.5412  0.1161\n",
            "Epoch 423, Loss G:  65.0720, Loss D:  0.3088  g_loss mse comp   2.4984  0.1126\n",
            "Epoch 424, Loss G:  66.2780, Loss D:  0.3134  g_loss mse comp   2.5445  0.1202\n",
            "Epoch 425, Loss G:  68.0289, Loss D:  0.3065  g_loss mse comp   2.6115  0.1297\n",
            "Epoch 426, Loss G:  65.6909, Loss D:  0.3228  g_loss mse comp   2.5222  0.1140\n",
            "Epoch 427, Loss G:  65.5074, Loss D:  0.3125  g_loss mse comp   2.5152  0.1120\n",
            "Epoch 428, Loss G:  67.2655, Loss D:  0.3191  g_loss mse comp   2.5823  0.1260\n",
            "Epoch 429, Loss G:  66.4004, Loss D:  0.3187  g_loss mse comp   2.5491  0.1228\n",
            "Epoch 430, Loss G:  66.9634, Loss D:  0.3302  g_loss mse comp   2.5708  0.1221\n",
            "Epoch 431, Loss G:  66.2820, Loss D:  0.3254  g_loss mse comp   2.5448  0.1180\n",
            "Epoch 432, Loss G:  65.0192, Loss D:  0.3315  g_loss mse comp   2.4965  0.1114\n",
            "Epoch 433, Loss G:  65.7909, Loss D:  0.3104  g_loss mse comp   2.5260  0.1152\n",
            "Epoch 434, Loss G:  66.7996, Loss D:  0.3297  g_loss mse comp   2.5643  0.1272\n",
            "Epoch 435, Loss G:  66.9926, Loss D:  0.3256  g_loss mse comp   2.5719  0.1222\n",
            "Epoch 436, Loss G:  65.9426, Loss D:  0.3200  g_loss mse comp   2.5318  0.1169\n",
            "Epoch 437, Loss G:  66.6513, Loss D:  0.3181  g_loss mse comp   2.5589  0.1199\n",
            "Epoch 438, Loss G:  65.8434, Loss D:  0.3227  g_loss mse comp   2.5282  0.1113\n",
            "Epoch 439, Loss G:  65.0428, Loss D:  0.3228  g_loss mse comp   2.4973  0.1133\n",
            "Epoch 440, Loss G:  66.2824, Loss D:  0.3221  g_loss mse comp   2.5448  0.1185\n",
            "Epoch 441, Loss G:  65.6572, Loss D:  0.3302  g_loss mse comp   2.5207  0.1182\n",
            "Epoch 442, Loss G:  66.9745, Loss D:  0.3247  g_loss mse comp   2.5713  0.1201\n",
            "Epoch 443, Loss G:  65.9621, Loss D:  0.3042  g_loss mse comp   2.5324  0.1185\n",
            "Epoch 444, Loss G:  67.0432, Loss D:  0.3284  g_loss mse comp   2.5738  0.1249\n",
            "Epoch 445, Loss G:  66.2646, Loss D:  0.3132  g_loss mse comp   2.5442  0.1149\n",
            "Epoch 446, Loss G:  64.6008, Loss D:  0.3199  g_loss mse comp   2.4805  0.1082\n",
            "Epoch 447, Loss G:  66.3799, Loss D:  0.3181  g_loss mse comp   2.5484  0.1208\n",
            "Epoch 448, Loss G:  64.7335, Loss D:  0.3214  g_loss mse comp   2.4855  0.1117\n",
            "Epoch 449, Loss G:  66.9522, Loss D:  0.3172  g_loss mse comp   2.5705  0.1188\n",
            "Epoch 450, Loss G:  65.4418, Loss D:  0.3255  g_loss mse comp   2.5126  0.1154\n",
            "Epoch 451, Loss G:  66.7606, Loss D:  0.3298  g_loss mse comp   2.5631  0.1193\n",
            "Epoch 452, Loss G:  65.3752, Loss D:  0.3156  g_loss mse comp   2.5100  0.1149\n",
            "Epoch 453, Loss G:  66.7064, Loss D:  0.3193  g_loss mse comp   2.5610  0.1201\n",
            "Epoch 454, Loss G:  64.1890, Loss D:  0.3111  g_loss mse comp   2.4647  0.1058\n",
            "Epoch 455, Loss G:  65.3081, Loss D:  0.3097  g_loss mse comp   2.5076  0.1100\n",
            "Epoch 456, Loss G:  66.7110, Loss D:  0.3150  g_loss mse comp   2.5609  0.1270\n",
            "Epoch 457, Loss G:  65.5240, Loss D:  0.3185  g_loss mse comp   2.5157  0.1166\n",
            "Epoch 458, Loss G:  65.9127, Loss D:  0.3139  g_loss mse comp   2.5307  0.1140\n",
            "Epoch 459, Loss G:  63.7324, Loss D:  0.3170  g_loss mse comp   2.4473  0.1029\n",
            "Epoch 460, Loss G:  66.2850, Loss D:  0.3151  g_loss mse comp   2.5448  0.1206\n",
            "Epoch 461, Loss G:  66.5190, Loss D:  0.3145  g_loss mse comp   2.5537  0.1225\n",
            "Epoch 462, Loss G:  65.1978, Loss D:  0.3241  g_loss mse comp   2.5032  0.1141\n",
            "Epoch 463, Loss G:  66.1263, Loss D:  0.3171  g_loss mse comp   2.5389  0.1153\n",
            "Epoch 464, Loss G:  65.9882, Loss D:  0.3262  g_loss mse comp   2.5334  0.1193\n",
            "Epoch 465, Loss G:  66.8742, Loss D:  0.3216  g_loss mse comp   2.5673  0.1257\n",
            "Epoch 466, Loss G:  66.1777, Loss D:  0.3081  g_loss mse comp   2.5409  0.1155\n",
            "Epoch 467, Loss G:  65.4277, Loss D:  0.3366  g_loss mse comp   2.5120  0.1146\n",
            "Epoch 468, Loss G:  66.7043, Loss D:  0.3314  g_loss mse comp   2.5610  0.1196\n",
            "Epoch 469, Loss G:  65.7958, Loss D:  0.3090  g_loss mse comp   2.5260  0.1187\n",
            "Epoch 470, Loss G:  65.2500, Loss D:  0.3237  g_loss mse comp   2.5053  0.1116\n",
            "Epoch 471, Loss G:  66.7652, Loss D:  0.3134  g_loss mse comp   2.5631  0.1237\n",
            "Epoch 472, Loss G:  65.7344, Loss D:  0.3285  g_loss mse comp   2.5239  0.1126\n",
            "Epoch 473, Loss G:  65.4953, Loss D:  0.3179  g_loss mse comp   2.5148  0.1103\n",
            "Epoch 474, Loss G:  65.2910, Loss D:  0.3182  g_loss mse comp   2.5068  0.1142\n",
            "Epoch 475, Loss G:  66.5891, Loss D:  0.3188  g_loss mse comp   2.5564  0.1224\n",
            "Epoch 476, Loss G:  66.9320, Loss D:  0.3305  g_loss mse comp   2.5696  0.1227\n",
            "Epoch 477, Loss G:  67.2398, Loss D:  0.3319  g_loss mse comp   2.5813  0.1248\n",
            "Epoch 478, Loss G:  66.4065, Loss D:  0.3240  g_loss mse comp   2.5495  0.1185\n",
            "Epoch 479, Loss G:  67.9045, Loss D:  0.3318  g_loss mse comp   2.6066  0.1323\n",
            "Epoch 480, Loss G:  67.1793, Loss D:  0.3230  g_loss mse comp   2.5791  0.1236\n",
            "Epoch 481, Loss G:  65.1787, Loss D:  0.3107  g_loss mse comp   2.5025  0.1139\n",
            "Epoch 482, Loss G:  65.9862, Loss D:  0.3195  g_loss mse comp   2.5336  0.1128\n",
            "Epoch 483, Loss G:  66.2513, Loss D:  0.3092  g_loss mse comp   2.5437  0.1148\n",
            "Epoch 484, Loss G:  65.3506, Loss D:  0.3078  g_loss mse comp   2.5093  0.1097\n",
            "Epoch 485, Loss G:  65.4346, Loss D:  0.3222  g_loss mse comp   2.5123  0.1144\n",
            "Epoch 486, Loss G:  67.2174, Loss D:  0.3202  g_loss mse comp   2.5806  0.1226\n",
            "Epoch 487, Loss G:  65.0003, Loss D:  0.3197  g_loss mse comp   2.4957  0.1116\n",
            "Epoch 488, Loss G:  66.4808, Loss D:  0.3209  g_loss mse comp   2.5523  0.1217\n",
            "Epoch 489, Loss G:  67.2546, Loss D:  0.3192  g_loss mse comp   2.5820  0.1228\n",
            "Epoch 490, Loss G:  66.9259, Loss D:  0.3358  g_loss mse comp   2.5692  0.1264\n",
            "Epoch 491, Loss G:  66.2447, Loss D:  0.3208  g_loss mse comp   2.5431  0.1253\n",
            "Epoch 492, Loss G:  66.3792, Loss D:  0.3137  g_loss mse comp   2.5486  0.1152\n",
            "Epoch 493, Loss G:  65.8503, Loss D:  0.3122  g_loss mse comp   2.5281  0.1209\n",
            "Epoch 494, Loss G:  65.5714, Loss D:  0.3217  g_loss mse comp   2.5175  0.1156\n",
            "Epoch 495, Loss G:  66.9589, Loss D:  0.3065  g_loss mse comp   2.5707  0.1216\n",
            "Epoch 496, Loss G:  66.6579, Loss D:  0.3186  g_loss mse comp   2.5590  0.1250\n",
            "Epoch 497, Loss G:  66.9924, Loss D:  0.3163  g_loss mse comp   2.5718  0.1256\n",
            "Epoch 498, Loss G:  66.8694, Loss D:  0.3130  g_loss mse comp   2.5672  0.1216\n",
            "Epoch 499, Loss G:  66.7304, Loss D:  0.3077  g_loss mse comp   2.5618  0.1236\n",
            "Epoch 500, Loss G:  66.2464, Loss D:  0.3084  g_loss mse comp   2.5434  0.1171\n",
            "Epoch 501, Loss G:  67.5699, Loss D:  0.3274  g_loss mse comp   2.5940  0.1246\n",
            "Epoch 502, Loss G:  67.2709, Loss D:  0.3217  g_loss mse comp   2.5826  0.1220\n",
            "Epoch 503, Loss G:  65.8370, Loss D:  0.3062  g_loss mse comp   2.5278  0.1135\n",
            "Epoch 504, Loss G:  66.7357, Loss D:  0.3260  g_loss mse comp   2.5621  0.1208\n",
            "Epoch 505, Loss G:  66.3009, Loss D:  0.3175  g_loss mse comp   2.5454  0.1195\n",
            "Epoch 506, Loss G:  66.3703, Loss D:  0.3332  g_loss mse comp   2.5480  0.1216\n",
            "Epoch 507, Loss G:  65.8929, Loss D:  0.3168  g_loss mse comp   2.5298  0.1192\n",
            "Epoch 508, Loss G:  64.9118, Loss D:  0.3205  g_loss mse comp   2.4923  0.1131\n",
            "Epoch 509, Loss G:  65.6780, Loss D:  0.3162  g_loss mse comp   2.5215  0.1193\n",
            "Epoch 510, Loss G:  66.0288, Loss D:  0.3140  g_loss mse comp   2.5350  0.1195\n",
            "Epoch 511, Loss G:  67.2366, Loss D:  0.3267  g_loss mse comp   2.5813  0.1218\n",
            "Epoch 512, Loss G:  66.1040, Loss D:  0.3277  g_loss mse comp   2.5379  0.1182\n",
            "Epoch 513, Loss G:  65.8349, Loss D:  0.3055  g_loss mse comp   2.5274  0.1213\n",
            "Epoch 514, Loss G:  66.1414, Loss D:  0.3055  g_loss mse comp   2.5393  0.1192\n",
            "Epoch 515, Loss G:  65.0442, Loss D:  0.3203  g_loss mse comp   2.4972  0.1169\n",
            "Epoch 516, Loss G:  67.4347, Loss D:  0.3301  g_loss mse comp   2.5886  0.1308\n",
            "Epoch 517, Loss G:  66.1330, Loss D:  0.3134  g_loss mse comp   2.5389  0.1225\n",
            "Epoch 518, Loss G:  65.3284, Loss D:  0.3364  g_loss mse comp   2.5084  0.1101\n",
            "Epoch 519, Loss G:  66.0464, Loss D:  0.3110  g_loss mse comp   2.5358  0.1151\n",
            "Epoch 520, Loss G:  65.1398, Loss D:  0.3309  g_loss mse comp   2.5010  0.1137\n",
            "Epoch 521, Loss G:  65.2052, Loss D:  0.3135  g_loss mse comp   2.5036  0.1108\n",
            "Epoch 522, Loss G:  66.1656, Loss D:  0.3110  g_loss mse comp   2.5403  0.1177\n",
            "Epoch 523, Loss G:  65.3750, Loss D:  0.3122  g_loss mse comp   2.5100  0.1142\n",
            "Epoch 524, Loss G:  65.2994, Loss D:  0.3047  g_loss mse comp   2.5071  0.1136\n",
            "Epoch 525, Loss G:  64.7990, Loss D:  0.3158  g_loss mse comp   2.4881  0.1075\n",
            "Epoch 526, Loss G:  66.1616, Loss D:  0.3289  g_loss mse comp   2.5399  0.1232\n",
            "Epoch 527, Loss G:  66.5967, Loss D:  0.3227  g_loss mse comp   2.5567  0.1219\n",
            "Epoch 528, Loss G:  66.9489, Loss D:  0.2982  g_loss mse comp   2.5702  0.1245\n",
            "Epoch 529, Loss G:  66.4948, Loss D:  0.3139  g_loss mse comp   2.5529  0.1192\n",
            "Epoch 530, Loss G:  66.2925, Loss D:  0.3130  g_loss mse comp   2.5452  0.1178\n",
            "Epoch 531, Loss G:  65.9921, Loss D:  0.3088  g_loss mse comp   2.5337  0.1150\n",
            "Epoch 532, Loss G:  65.2059, Loss D:  0.3022  g_loss mse comp   2.5036  0.1115\n",
            "Epoch 533, Loss G:  65.8787, Loss D:  0.3204  g_loss mse comp   2.5292  0.1197\n",
            "Epoch 534, Loss G:  65.3024, Loss D:  0.3269  g_loss mse comp   2.5073  0.1138\n",
            "Epoch 535, Loss G:  66.6628, Loss D:  0.3169  g_loss mse comp   2.5593  0.1214\n",
            "Epoch 536, Loss G:  66.5563, Loss D:  0.3217  g_loss mse comp   2.5552  0.1219\n",
            "Epoch 537, Loss G:  66.5865, Loss D:  0.3065  g_loss mse comp   2.5561  0.1278\n",
            "Epoch 538, Loss G:  66.3380, Loss D:  0.3178  g_loss mse comp   2.5468  0.1208\n",
            "Epoch 539, Loss G:  64.2106, Loss D:  0.3175  g_loss mse comp   2.4656  0.1056\n",
            "Epoch 540, Loss G:  65.7791, Loss D:  0.3202  g_loss mse comp   2.5254  0.1195\n",
            "Epoch 541, Loss G:  66.6691, Loss D:  0.3180  g_loss mse comp   2.5597  0.1177\n",
            "Epoch 542, Loss G:  65.7524, Loss D:  0.3155  g_loss mse comp   2.5245  0.1145\n",
            "Epoch 543, Loss G:  66.3472, Loss D:  0.3239  g_loss mse comp   2.5472  0.1194\n",
            "Epoch 544, Loss G:  65.3373, Loss D:  0.3225  g_loss mse comp   2.5085  0.1172\n",
            "Epoch 545, Loss G:  65.5958, Loss D:  0.3222  g_loss mse comp   2.5183  0.1211\n",
            "Epoch 546, Loss G:  66.3158, Loss D:  0.3025  g_loss mse comp   2.5460  0.1189\n",
            "Epoch 547, Loss G:  64.4394, Loss D:  0.3249  g_loss mse comp   2.4742  0.1091\n",
            "Epoch 548, Loss G:  66.1539, Loss D:  0.3096  g_loss mse comp   2.5398  0.1191\n",
            "Epoch 549, Loss G:  65.4469, Loss D:  0.3221  g_loss mse comp   2.5128  0.1152\n",
            "Epoch 550, Loss G:  66.0961, Loss D:  0.3120  g_loss mse comp   2.5376  0.1188\n",
            "Epoch 551, Loss G:  65.6005, Loss D:  0.3051  g_loss mse comp   2.5186  0.1177\n",
            "Epoch 552, Loss G:  65.7300, Loss D:  0.3212  g_loss mse comp   2.5236  0.1154\n",
            "Epoch 553, Loss G:  66.0804, Loss D:  0.3127  g_loss mse comp   2.5370  0.1181\n",
            "Epoch 554, Loss G:  65.9701, Loss D:  0.3129  g_loss mse comp   2.5326  0.1230\n",
            "Epoch 555, Loss G:  66.1211, Loss D:  0.3187  g_loss mse comp   2.5386  0.1175\n",
            "Epoch 556, Loss G:  65.1776, Loss D:  0.3146  g_loss mse comp   2.5023  0.1191\n",
            "Epoch 557, Loss G:  66.2428, Loss D:  0.3075  g_loss mse comp   2.5431  0.1224\n",
            "Epoch 558, Loss G:  65.2101, Loss D:  0.3132  g_loss mse comp   2.5037  0.1136\n",
            "Epoch 559, Loss G:  64.6475, Loss D:  0.3210  g_loss mse comp   2.4823  0.1081\n",
            "Epoch 560, Loss G:  66.8006, Loss D:  0.3198  g_loss mse comp   2.5646  0.1217\n",
            "Epoch 561, Loss G:  66.0798, Loss D:  0.3241  g_loss mse comp   2.5368  0.1218\n",
            "Epoch 562, Loss G:  66.1694, Loss D:  0.3120  g_loss mse comp   2.5404  0.1181\n",
            "Epoch 563, Loss G:  66.6593, Loss D:  0.3247  g_loss mse comp   2.5590  0.1253\n",
            "Epoch 564, Loss G:  66.3778, Loss D:  0.3115  g_loss mse comp   2.5482  0.1246\n",
            "Epoch 565, Loss G:  66.2846, Loss D:  0.3212  g_loss mse comp   2.5448  0.1188\n",
            "Epoch 566, Loss G:  64.8814, Loss D:  0.3149  g_loss mse comp   2.4913  0.1069\n",
            "Epoch 567, Loss G:  66.2768, Loss D:  0.3200  g_loss mse comp   2.5444  0.1220\n",
            "Epoch 568, Loss G:  65.7840, Loss D:  0.3068  g_loss mse comp   2.5258  0.1133\n",
            "Epoch 569, Loss G:  65.9059, Loss D:  0.3223  g_loss mse comp   2.5304  0.1157\n",
            "Epoch 570, Loss G:  66.3627, Loss D:  0.3171  g_loss mse comp   2.5478  0.1208\n",
            "Epoch 571, Loss G:  64.8141, Loss D:  0.3326  g_loss mse comp   2.4885  0.1131\n",
            "Epoch 572, Loss G:  66.5152, Loss D:  0.3289  g_loss mse comp   2.5536  0.1221\n",
            "Epoch 573, Loss G:  67.1349, Loss D:  0.3236  g_loss mse comp   2.5770  0.1324\n",
            "Epoch 574, Loss G:  63.9398, Loss D:  0.3248  g_loss mse comp   2.4549  0.1129\n",
            "Epoch 575, Loss G:  65.9723, Loss D:  0.3264  g_loss mse comp   2.5325  0.1279\n",
            "Epoch 576, Loss G:  65.0509, Loss D:  0.3061  g_loss mse comp   2.4976  0.1135\n",
            "Epoch 577, Loss G:  66.0293, Loss D:  0.3178  g_loss mse comp   2.5348  0.1243\n",
            "Epoch 578, Loss G:  66.1524, Loss D:  0.3058  g_loss mse comp   2.5396  0.1232\n",
            "Epoch 579, Loss G:  65.5117, Loss D:  0.3107  g_loss mse comp   2.5153  0.1142\n",
            "Epoch 580, Loss G:  66.4894, Loss D:  0.3218  g_loss mse comp   2.5524  0.1275\n",
            "Epoch 581, Loss G:  65.7426, Loss D:  0.2986  g_loss mse comp   2.5238  0.1240\n",
            "Epoch 582, Loss G:  66.1094, Loss D:  0.3018  g_loss mse comp   2.5378  0.1257\n",
            "Epoch 583, Loss G:  67.5252, Loss D:  0.3271  g_loss mse comp   2.5919  0.1362\n",
            "Epoch 584, Loss G:  65.4114, Loss D:  0.3186  g_loss mse comp   2.5114  0.1148\n",
            "Epoch 585, Loss G:  65.5224, Loss D:  0.3175  g_loss mse comp   2.5154  0.1218\n",
            "Epoch 586, Loss G:  65.7219, Loss D:  0.3226  g_loss mse comp   2.5230  0.1230\n",
            "Epoch 587, Loss G:  66.7196, Loss D:  0.3051  g_loss mse comp   2.5612  0.1273\n",
            "Epoch 588, Loss G:  66.0800, Loss D:  0.3258  g_loss mse comp   2.5366  0.1273\n",
            "Epoch 589, Loss G:  66.9708, Loss D:  0.3088  g_loss mse comp   2.5709  0.1270\n",
            "Epoch 590, Loss G:  65.3572, Loss D:  0.3142  g_loss mse comp   2.5093  0.1164\n",
            "Epoch 591, Loss G:  66.5474, Loss D:  0.3297  g_loss mse comp   2.5545  0.1307\n",
            "Epoch 592, Loss G:  64.5422, Loss D:  0.3081  g_loss mse comp   2.4782  0.1088\n",
            "Epoch 593, Loss G:  66.7440, Loss D:  0.3168  g_loss mse comp   2.5621  0.1298\n",
            "Epoch 594, Loss G:  66.7721, Loss D:  0.3192  g_loss mse comp   2.5630  0.1346\n",
            "Epoch 595, Loss G:  65.4202, Loss D:  0.3132  g_loss mse comp   2.5115  0.1209\n",
            "Epoch 596, Loss G:  65.5764, Loss D:  0.3105  g_loss mse comp   2.5174  0.1230\n",
            "Epoch 597, Loss G:  65.9747, Loss D:  0.3138  g_loss mse comp   2.5328  0.1228\n",
            "Epoch 598, Loss G:  66.6331, Loss D:  0.3127  g_loss mse comp   2.5579  0.1277\n",
            "Epoch 599, Loss G:  66.0818, Loss D:  0.3045  g_loss mse comp   2.5370  0.1203\n",
            "Epoch 600, Loss G:  67.3840, Loss D:  0.3048  g_loss mse comp   2.5868  0.1282\n",
            "Epoch 601, Loss G:  66.4874, Loss D:  0.3005  g_loss mse comp   2.5522  0.1291\n",
            "Epoch 602, Loss G:  65.8701, Loss D:  0.2910  g_loss mse comp   2.5288  0.1211\n",
            "Epoch 603, Loss G:  65.6624, Loss D:  0.3106  g_loss mse comp   2.5206  0.1255\n",
            "Epoch 604, Loss G:  65.6814, Loss D:  0.3115  g_loss mse comp   2.5216  0.1197\n",
            "Epoch 605, Loss G:  67.2280, Loss D:  0.3078  g_loss mse comp   2.5806  0.1325\n",
            "Epoch 606, Loss G:  66.7448, Loss D:  0.3051  g_loss mse comp   2.5621  0.1310\n",
            "Epoch 607, Loss G:  67.0348, Loss D:  0.3175  g_loss mse comp   2.5732  0.1308\n",
            "Epoch 608, Loss G:  66.2765, Loss D:  0.3078  g_loss mse comp   2.5444  0.1217\n",
            "Epoch 609, Loss G:  65.0723, Loss D:  0.3070  g_loss mse comp   2.4981  0.1208\n",
            "Epoch 610, Loss G:  67.3086, Loss D:  0.3230  g_loss mse comp   2.5834  0.1398\n",
            "Epoch 611, Loss G:  66.2951, Loss D:  0.3097  g_loss mse comp   2.5448  0.1296\n",
            "Epoch 612, Loss G:  65.8269, Loss D:  0.2979  g_loss mse comp   2.5270  0.1237\n",
            "Epoch 613, Loss G:  65.8418, Loss D:  0.3119  g_loss mse comp   2.5277  0.1211\n",
            "Epoch 614, Loss G:  65.9913, Loss D:  0.3091  g_loss mse comp   2.5332  0.1279\n",
            "Epoch 615, Loss G:  66.2257, Loss D:  0.3222  g_loss mse comp   2.5422  0.1273\n",
            "Epoch 616, Loss G:  65.8226, Loss D:  0.2977  g_loss mse comp   2.5269  0.1230\n",
            "Epoch 617, Loss G:  65.7660, Loss D:  0.3087  g_loss mse comp   2.5246  0.1273\n",
            "Epoch 618, Loss G:  65.3193, Loss D:  0.3002  g_loss mse comp   2.5078  0.1160\n",
            "Epoch 619, Loss G:  67.1144, Loss D:  0.3100  g_loss mse comp   2.5761  0.1353\n",
            "Epoch 620, Loss G:  66.2600, Loss D:  0.3110  g_loss mse comp   2.5434  0.1304\n",
            "Epoch 621, Loss G:  65.0435, Loss D:  0.3115  g_loss mse comp   2.4971  0.1188\n",
            "Epoch 622, Loss G:  66.0728, Loss D:  0.3190  g_loss mse comp   2.5363  0.1281\n",
            "Epoch 623, Loss G:  66.8264, Loss D:  0.3062  g_loss mse comp   2.5653  0.1287\n",
            "Epoch 624, Loss G:  65.2909, Loss D:  0.3043  g_loss mse comp   2.5065  0.1211\n",
            "Epoch 625, Loss G:  65.1781, Loss D:  0.3037  g_loss mse comp   2.5021  0.1226\n",
            "Epoch 626, Loss G:  65.4684, Loss D:  0.2967  g_loss mse comp   2.5131  0.1280\n",
            "Epoch 627, Loss G:  67.1986, Loss D:  0.3118  g_loss mse comp   2.5793  0.1360\n",
            "Epoch 628, Loss G:  64.3435, Loss D:  0.3080  g_loss mse comp   2.4703  0.1167\n",
            "Epoch 629, Loss G:  66.1662, Loss D:  0.2925  g_loss mse comp   2.5397  0.1347\n",
            "Epoch 630, Loss G:  67.2132, Loss D:  0.3015  g_loss mse comp   2.5798  0.1395\n",
            "Epoch 631, Loss G:  65.9000, Loss D:  0.3045  g_loss mse comp   2.5297  0.1270\n",
            "Epoch 632, Loss G:  65.1661, Loss D:  0.3162  g_loss mse comp   2.5017  0.1208\n",
            "Epoch 633, Loss G:  67.6670, Loss D:  0.3049  g_loss mse comp   2.5974  0.1351\n",
            "Epoch 634, Loss G:  65.4923, Loss D:  0.3021  g_loss mse comp   2.5141  0.1267\n",
            "Epoch 635, Loss G:  65.9367, Loss D:  0.3056  g_loss mse comp   2.5309  0.1337\n",
            "Epoch 636, Loss G:  65.6261, Loss D:  0.3139  g_loss mse comp   2.5189  0.1358\n",
            "Epoch 637, Loss G:  66.5263, Loss D:  0.2980  g_loss mse comp   2.5534  0.1378\n",
            "Epoch 638, Loss G:  66.2437, Loss D:  0.3128  g_loss mse comp   2.5429  0.1280\n",
            "Epoch 639, Loss G:  66.8144, Loss D:  0.3122  g_loss mse comp   2.5645  0.1378\n",
            "Epoch 640, Loss G:  66.0051, Loss D:  0.3154  g_loss mse comp   2.5334  0.1355\n",
            "Epoch 641, Loss G:  66.7732, Loss D:  0.2957  g_loss mse comp   2.5629  0.1388\n",
            "Epoch 642, Loss G:  65.5644, Loss D:  0.3013  g_loss mse comp   2.5166  0.1338\n",
            "Epoch 643, Loss G:  65.9709, Loss D:  0.3085  g_loss mse comp   2.5323  0.1314\n",
            "Epoch 644, Loss G:  66.1173, Loss D:  0.3012  g_loss mse comp   2.5379  0.1307\n",
            "Epoch 645, Loss G:  66.1168, Loss D:  0.2919  g_loss mse comp   2.5380  0.1291\n",
            "Epoch 646, Loss G:  66.2001, Loss D:  0.3196  g_loss mse comp   2.5409  0.1370\n",
            "Epoch 647, Loss G:  67.8265, Loss D:  0.3036  g_loss mse comp   2.6032  0.1432\n",
            "Epoch 648, Loss G:  65.3024, Loss D:  0.3095  g_loss mse comp   2.5068  0.1246\n",
            "Epoch 649, Loss G:  65.5580, Loss D:  0.3107  g_loss mse comp   2.5163  0.1342\n",
            "Epoch 650, Loss G:  65.9552, Loss D:  0.3124  g_loss mse comp   2.5318  0.1295\n",
            "Epoch 651, Loss G:  65.9463, Loss D:  0.3000  g_loss mse comp   2.5312  0.1357\n",
            "Epoch 652, Loss G:  65.8328, Loss D:  0.3116  g_loss mse comp   2.5270  0.1320\n",
            "Epoch 653, Loss G:  63.4477, Loss D:  0.2872  g_loss mse comp   2.4360  0.1107\n",
            "Epoch 654, Loss G:  66.4252, Loss D:  0.2797  g_loss mse comp   2.5496  0.1353\n",
            "Epoch 655, Loss G:  67.1381, Loss D:  0.3081  g_loss mse comp   2.5769  0.1391\n",
            "Epoch 656, Loss G:  65.3604, Loss D:  0.2980  g_loss mse comp   2.5090  0.1267\n",
            "Epoch 657, Loss G:  65.9182, Loss D:  0.2985  g_loss mse comp   2.5306  0.1236\n",
            "Epoch 658, Loss G:  66.0441, Loss D:  0.2782  g_loss mse comp   2.5350  0.1334\n",
            "Epoch 659, Loss G:  65.8085, Loss D:  0.2978  g_loss mse comp   2.5261  0.1304\n",
            "Epoch 660, Loss G:  66.3313, Loss D:  0.2832  g_loss mse comp   2.5461  0.1315\n",
            "Epoch 661, Loss G:  66.5772, Loss D:  0.3051  g_loss mse comp   2.5552  0.1411\n",
            "Epoch 662, Loss G:  65.8413, Loss D:  0.3062  g_loss mse comp   2.5271  0.1373\n",
            "Epoch 663, Loss G:  67.2135, Loss D:  0.3033  g_loss mse comp   2.5797  0.1408\n",
            "Epoch 664, Loss G:  65.0306, Loss D:  0.2850  g_loss mse comp   2.4962  0.1298\n",
            "Epoch 665, Loss G:  67.7958, Loss D:  0.3029  g_loss mse comp   2.6020  0.1445\n",
            "Epoch 666, Loss G:  64.9579, Loss D:  0.3212  g_loss mse comp   2.4937  0.1229\n",
            "Epoch 667, Loss G:  65.4758, Loss D:  0.2854  g_loss mse comp   2.5132  0.1331\n",
            "Epoch 668, Loss G:  66.4113, Loss D:  0.3227  g_loss mse comp   2.5490  0.1361\n",
            "Epoch 669, Loss G:  66.5165, Loss D:  0.2897  g_loss mse comp   2.5529  0.1406\n",
            "Epoch 670, Loss G:  65.4896, Loss D:  0.3066  g_loss mse comp   2.5137  0.1339\n",
            "Epoch 671, Loss G:  66.5712, Loss D:  0.2976  g_loss mse comp   2.5551  0.1396\n",
            "Epoch 672, Loss G:  65.8940, Loss D:  0.2891  g_loss mse comp   2.5293  0.1325\n",
            "Epoch 673, Loss G:  66.5938, Loss D:  0.3112  g_loss mse comp   2.5560  0.1386\n",
            "Epoch 674, Loss G:  66.2963, Loss D:  0.3051  g_loss mse comp   2.5445  0.1380\n",
            "Epoch 675, Loss G:  65.7753, Loss D:  0.3069  g_loss mse comp   2.5247  0.1337\n",
            "Epoch 676, Loss G:  67.6397, Loss D:  0.3006  g_loss mse comp   2.5959  0.1455\n",
            "Epoch 677, Loss G:  66.0635, Loss D:  0.2978  g_loss mse comp   2.5359  0.1296\n",
            "Epoch 678, Loss G:  65.5659, Loss D:  0.2917  g_loss mse comp   2.5167  0.1320\n",
            "Epoch 679, Loss G:  65.5749, Loss D:  0.2998  g_loss mse comp   2.5171  0.1308\n",
            "Epoch 680, Loss G:  64.9211, Loss D:  0.3094  g_loss mse comp   2.4922  0.1240\n",
            "Epoch 681, Loss G:  65.3004, Loss D:  0.3041  g_loss mse comp   2.5065  0.1327\n",
            "Epoch 682, Loss G:  66.7469, Loss D:  0.2893  g_loss mse comp   2.5616  0.1455\n",
            "Epoch 683, Loss G:  67.2336, Loss D:  0.2987  g_loss mse comp   2.5803  0.1456\n",
            "Epoch 684, Loss G:  65.7546, Loss D:  0.3022  g_loss mse comp   2.5237  0.1386\n",
            "Epoch 685, Loss G:  65.2286, Loss D:  0.2959  g_loss mse comp   2.5040  0.1259\n",
            "Epoch 686, Loss G:  65.9331, Loss D:  0.3068  g_loss mse comp   2.5307  0.1362\n",
            "Epoch 687, Loss G:  64.8341, Loss D:  0.2981  g_loss mse comp   2.4885  0.1325\n",
            "Epoch 688, Loss G:  65.9652, Loss D:  0.3029  g_loss mse comp   2.5320  0.1343\n",
            "Epoch 689, Loss G:  65.6371, Loss D:  0.3122  g_loss mse comp   2.5194  0.1328\n",
            "Epoch 690, Loss G:  65.9028, Loss D:  0.2974  g_loss mse comp   2.5295  0.1362\n",
            "Epoch 691, Loss G:  65.1252, Loss D:  0.2924  g_loss mse comp   2.4997  0.1337\n",
            "Epoch 692, Loss G:  65.6736, Loss D:  0.2943  g_loss mse comp   2.5209  0.1297\n",
            "Epoch 693, Loss G:  64.7318, Loss D:  0.3070  g_loss mse comp   2.4846  0.1314\n",
            "Epoch 694, Loss G:  66.3735, Loss D:  0.2963  g_loss mse comp   2.5476  0.1349\n",
            "Epoch 695, Loss G:  66.0641, Loss D:  0.3067  g_loss mse comp   2.5353  0.1450\n",
            "Epoch 696, Loss G:  66.4166, Loss D:  0.2985  g_loss mse comp   2.5490  0.1421\n",
            "Epoch 697, Loss G:  66.1772, Loss D:  0.3097  g_loss mse comp   2.5401  0.1354\n",
            "Epoch 698, Loss G:  66.4299, Loss D:  0.2755  g_loss mse comp   2.5497  0.1386\n",
            "Epoch 699, Loss G:  65.7757, Loss D:  0.3116  g_loss mse comp   2.5244  0.1403\n",
            "Epoch 700, Loss G:  65.9016, Loss D:  0.2985  g_loss mse comp   2.5290  0.1463\n",
            "Epoch 701, Loss G:  66.2663, Loss D:  0.2944  g_loss mse comp   2.5434  0.1368\n",
            "Epoch 702, Loss G:  64.9965, Loss D:  0.2867  g_loss mse comp   2.4947  0.1339\n",
            "Epoch 703, Loss G:  64.6334, Loss D:  0.2824  g_loss mse comp   2.4809  0.1306\n",
            "Epoch 704, Loss G:  64.7818, Loss D:  0.3061  g_loss mse comp   2.4866  0.1294\n",
            "Epoch 705, Loss G:  64.2075, Loss D:  0.2984  g_loss mse comp   2.4646  0.1270\n",
            "Epoch 706, Loss G:  65.3998, Loss D:  0.2982  g_loss mse comp   2.5104  0.1303\n",
            "Epoch 707, Loss G:  66.8062, Loss D:  0.2866  g_loss mse comp   2.5639  0.1450\n",
            "Epoch 708, Loss G:  66.3733, Loss D:  0.2851  g_loss mse comp   2.5474  0.1403\n",
            "Epoch 709, Loss G:  65.5254, Loss D:  0.2905  g_loss mse comp   2.5149  0.1385\n",
            "Epoch 710, Loss G:  66.5225, Loss D:  0.2924  g_loss mse comp   2.5531  0.1409\n",
            "Epoch 711, Loss G:  66.7923, Loss D:  0.3067  g_loss mse comp   2.5636  0.1399\n",
            "Epoch 712, Loss G:  64.3171, Loss D:  0.2995  g_loss mse comp   2.4689  0.1258\n",
            "Epoch 713, Loss G:  66.8303, Loss D:  0.2953  g_loss mse comp   2.5652  0.1342\n",
            "Epoch 714, Loss G:  66.1587, Loss D:  0.2847  g_loss mse comp   2.5393  0.1363\n",
            "Epoch 715, Loss G:  65.6033, Loss D:  0.3039  g_loss mse comp   2.5181  0.1330\n",
            "Epoch 716, Loss G:  66.9680, Loss D:  0.2927  g_loss mse comp   2.5701  0.1446\n",
            "Epoch 717, Loss G:  66.5641, Loss D:  0.2902  g_loss mse comp   2.5547  0.1423\n",
            "Epoch 718, Loss G:  64.8040, Loss D:  0.2885  g_loss mse comp   2.4872  0.1377\n",
            "Epoch 719, Loss G:  65.4254, Loss D:  0.3123  g_loss mse comp   2.5114  0.1301\n",
            "Epoch 720, Loss G:  67.2263, Loss D:  0.3064  g_loss mse comp   2.5798  0.1519\n",
            "Epoch 721, Loss G:  67.0700, Loss D:  0.2887  g_loss mse comp   2.5740  0.1456\n",
            "Epoch 722, Loss G:  67.6114, Loss D:  0.2910  g_loss mse comp   2.5946  0.1519\n",
            "Epoch 723, Loss G:  66.0458, Loss D:  0.2935  g_loss mse comp   2.5349  0.1376\n",
            "Epoch 724, Loss G:  66.5997, Loss D:  0.2951  g_loss mse comp   2.5561  0.1415\n",
            "Epoch 725, Loss G:  65.5485, Loss D:  0.2889  g_loss mse comp   2.5157  0.1408\n",
            "Epoch 726, Loss G:  66.3853, Loss D:  0.3008  g_loss mse comp   2.5478  0.1414\n",
            "Epoch 727, Loss G:  66.0196, Loss D:  0.2955  g_loss mse comp   2.5338  0.1420\n",
            "Epoch 728, Loss G:  65.7312, Loss D:  0.2965  g_loss mse comp   2.5227  0.1400\n",
            "Epoch 729, Loss G:  65.9666, Loss D:  0.2871  g_loss mse comp   2.5320  0.1348\n",
            "Epoch 730, Loss G:  66.6393, Loss D:  0.3076  g_loss mse comp   2.5578  0.1364\n",
            "Epoch 731, Loss G:  66.2505, Loss D:  0.2819  g_loss mse comp   2.5424  0.1474\n",
            "Epoch 732, Loss G:  67.3980, Loss D:  0.2907  g_loss mse comp   2.5863  0.1533\n",
            "Epoch 733, Loss G:  67.7018, Loss D:  0.2901  g_loss mse comp   2.5981  0.1505\n",
            "Epoch 734, Loss G:  66.8336, Loss D:  0.2930  g_loss mse comp   2.5650  0.1446\n",
            "Epoch 735, Loss G:  65.5485, Loss D:  0.2869  g_loss mse comp   2.5156  0.1421\n",
            "Epoch 736, Loss G:  65.9985, Loss D:  0.2815  g_loss mse comp   2.5331  0.1386\n",
            "Epoch 737, Loss G:  66.4267, Loss D:  0.2930  g_loss mse comp   2.5494  0.1429\n",
            "Epoch 738, Loss G:  66.1428, Loss D:  0.3045  g_loss mse comp   2.5384  0.1452\n",
            "Epoch 739, Loss G:  66.2242, Loss D:  0.2870  g_loss mse comp   2.5416  0.1430\n",
            "Epoch 740, Loss G:  65.8409, Loss D:  0.3167  g_loss mse comp   2.5269  0.1418\n",
            "Epoch 741, Loss G:  65.9518, Loss D:  0.2877  g_loss mse comp   2.5314  0.1360\n",
            "Epoch 742, Loss G:  67.2718, Loss D:  0.3331  g_loss mse comp   2.5816  0.1501\n",
            "Epoch 743, Loss G:  66.3602, Loss D:  0.3041  g_loss mse comp   2.5470  0.1375\n",
            "Epoch 744, Loss G:  65.4543, Loss D:  0.3006  g_loss mse comp   2.5125  0.1300\n",
            "Epoch 745, Loss G:  65.7248, Loss D:  0.3131  g_loss mse comp   2.5227  0.1353\n",
            "Epoch 746, Loss G:  64.9437, Loss D:  0.2853  g_loss mse comp   2.4925  0.1376\n",
            "Epoch 747, Loss G:  65.7840, Loss D:  0.3172  g_loss mse comp   2.5248  0.1388\n",
            "Epoch 748, Loss G:  66.5775, Loss D:  0.2884  g_loss mse comp   2.5552  0.1432\n",
            "Epoch 749, Loss G:  66.3886, Loss D:  0.3023  g_loss mse comp   2.5478  0.1467\n",
            "Epoch 750, Loss G:  65.5941, Loss D:  0.2850  g_loss mse comp   2.5176  0.1372\n",
            "Epoch 751, Loss G:  66.3111, Loss D:  0.3007  g_loss mse comp   2.5449  0.1441\n",
            "Epoch 752, Loss G:  66.0309, Loss D:  0.2946  g_loss mse comp   2.5343  0.1389\n",
            "Epoch 753, Loss G:  64.6810, Loss D:  0.2887  g_loss mse comp   2.4827  0.1295\n",
            "Epoch 754, Loss G:  66.4796, Loss D:  0.3017  g_loss mse comp   2.5514  0.1435\n",
            "Epoch 755, Loss G:  65.3905, Loss D:  0.2892  g_loss mse comp   2.5097  0.1374\n",
            "Epoch 756, Loss G:  65.4104, Loss D:  0.2938  g_loss mse comp   2.5103  0.1418\n",
            "Epoch 757, Loss G:  66.6096, Loss D:  0.2991  g_loss mse comp   2.5563  0.1457\n",
            "Epoch 758, Loss G:  66.1992, Loss D:  0.2915  g_loss mse comp   2.5408  0.1393\n",
            "Epoch 759, Loss G:  65.3161, Loss D:  0.3006  g_loss mse comp   2.5068  0.1399\n",
            "Epoch 760, Loss G:  64.6797, Loss D:  0.3071  g_loss mse comp   2.4826  0.1321\n",
            "Epoch 761, Loss G:  65.9180, Loss D:  0.2715  g_loss mse comp   2.5299  0.1398\n",
            "Epoch 762, Loss G:  65.1888, Loss D:  0.2781  g_loss mse comp   2.5021  0.1343\n",
            "Epoch 763, Loss G:  65.4363, Loss D:  0.2961  g_loss mse comp   2.5118  0.1297\n",
            "Epoch 764, Loss G:  65.7550, Loss D:  0.2882  g_loss mse comp   2.5237  0.1391\n",
            "Epoch 765, Loss G:  66.6885, Loss D:  0.2965  g_loss mse comp   2.5594  0.1429\n",
            "Epoch 766, Loss G:  65.6308, Loss D:  0.2967  g_loss mse comp   2.5190  0.1373\n",
            "Epoch 767, Loss G:  66.4795, Loss D:  0.3013  g_loss mse comp   2.5516  0.1390\n",
            "Epoch 768, Loss G:  65.9074, Loss D:  0.2916  g_loss mse comp   2.5297  0.1364\n",
            "Epoch 769, Loss G:  64.6644, Loss D:  0.3018  g_loss mse comp   2.4821  0.1287\n",
            "Epoch 770, Loss G:  67.0991, Loss D:  0.2861  g_loss mse comp   2.5752  0.1447\n",
            "Epoch 771, Loss G:  65.2507, Loss D:  0.2915  g_loss mse comp   2.5044  0.1374\n",
            "Epoch 772, Loss G:  65.7773, Loss D:  0.3011  g_loss mse comp   2.5245  0.1402\n",
            "Epoch 773, Loss G:  66.1463, Loss D:  0.3058  g_loss mse comp   2.5386  0.1432\n",
            "Epoch 774, Loss G:  66.0860, Loss D:  0.2918  g_loss mse comp   2.5366  0.1356\n",
            "Epoch 775, Loss G:  65.3645, Loss D:  0.2920  g_loss mse comp   2.5087  0.1372\n",
            "Epoch 776, Loss G:  65.1673, Loss D:  0.3122  g_loss mse comp   2.5013  0.1347\n",
            "Epoch 777, Loss G:  64.3234, Loss D:  0.3059  g_loss mse comp   2.4692  0.1235\n",
            "Epoch 778, Loss G:  66.5161, Loss D:  0.2789  g_loss mse comp   2.5526  0.1480\n",
            "Epoch 779, Loss G:  65.0784, Loss D:  0.2969  g_loss mse comp   2.4977  0.1374\n",
            "Epoch 780, Loss G:  65.8653, Loss D:  0.2840  g_loss mse comp   2.5281  0.1356\n",
            "Epoch 781, Loss G:  65.3862, Loss D:  0.2947  g_loss mse comp   2.5098  0.1314\n",
            "Epoch 782, Loss G:  65.4955, Loss D:  0.2914  g_loss mse comp   2.5137  0.1391\n",
            "Epoch 783, Loss G:  66.6014, Loss D:  0.3023  g_loss mse comp   2.5562  0.1397\n",
            "Epoch 784, Loss G:  65.9888, Loss D:  0.2964  g_loss mse comp   2.5326  0.1405\n",
            "Epoch 785, Loss G:  65.4540, Loss D:  0.2795  g_loss mse comp   2.5124  0.1324\n",
            "Epoch 786, Loss G:  66.0368, Loss D:  0.2912  g_loss mse comp   2.5347  0.1354\n",
            "Epoch 787, Loss G:  65.7818, Loss D:  0.2698  g_loss mse comp   2.5248  0.1378\n",
            "Epoch 788, Loss G:  65.8724, Loss D:  0.2873  g_loss mse comp   2.5283  0.1355\n",
            "Epoch 789, Loss G:  65.4997, Loss D:  0.2998  g_loss mse comp   2.5141  0.1325\n",
            "Epoch 790, Loss G:  64.8885, Loss D:  0.3077  g_loss mse comp   2.4906  0.1316\n",
            "Epoch 791, Loss G:  65.8717, Loss D:  0.3036  g_loss mse comp   2.5280  0.1446\n",
            "Epoch 792, Loss G:  66.6665, Loss D:  0.3062  g_loss mse comp   2.5584  0.1482\n",
            "Epoch 793, Loss G:  65.8017, Loss D:  0.2941  g_loss mse comp   2.5254  0.1420\n",
            "Epoch 794, Loss G:  65.2426, Loss D:  0.2782  g_loss mse comp   2.5040  0.1392\n",
            "Epoch 795, Loss G:  66.1918, Loss D:  0.2883  g_loss mse comp   2.5402  0.1457\n",
            "Epoch 796, Loss G:  65.2903, Loss D:  0.2882  g_loss mse comp   2.5060  0.1343\n",
            "Epoch 797, Loss G:  65.6090, Loss D:  0.2937  g_loss mse comp   2.5180  0.1420\n",
            "Epoch 798, Loss G:  64.6215, Loss D:  0.2933  g_loss mse comp   2.4804  0.1317\n",
            "Epoch 799, Loss G:  64.9535, Loss D:  0.2991  g_loss mse comp   2.4929  0.1384\n",
            "Epoch 800, Loss G:  65.9039, Loss D:  0.2827  g_loss mse comp   2.5293  0.1420\n",
            "Epoch 801, Loss G:  66.2398, Loss D:  0.2943  g_loss mse comp   2.5422  0.1434\n",
            "Epoch 802, Loss G:  66.8660, Loss D:  0.3048  g_loss mse comp   2.5660  0.1495\n",
            "Epoch 803, Loss G:  66.4075, Loss D:  0.2813  g_loss mse comp   2.5487  0.1401\n",
            "Epoch 804, Loss G:  66.3627, Loss D:  0.2754  g_loss mse comp   2.5464  0.1560\n",
            "Epoch 805, Loss G:  67.1356, Loss D:  0.2840  g_loss mse comp   2.5765  0.1477\n",
            "Epoch 806, Loss G:  66.7687, Loss D:  0.3010  g_loss mse comp   2.5623  0.1487\n",
            "Epoch 807, Loss G:  64.5532, Loss D:  0.2793  g_loss mse comp   2.4776  0.1354\n",
            "Epoch 808, Loss G:  66.4846, Loss D:  0.2985  g_loss mse comp   2.5512  0.1536\n",
            "Epoch 809, Loss G:  66.4033, Loss D:  0.2894  g_loss mse comp   2.5481  0.1519\n",
            "Epoch 810, Loss G:  65.8890, Loss D:  0.2959  g_loss mse comp   2.5285  0.1476\n",
            "Epoch 811, Loss G:  66.1903, Loss D:  0.2814  g_loss mse comp   2.5400  0.1498\n",
            "Epoch 812, Loss G:  64.8934, Loss D:  0.2982  g_loss mse comp   2.4908  0.1337\n",
            "Epoch 813, Loss G:  63.7924, Loss D:  0.2884  g_loss mse comp   2.4486  0.1275\n",
            "Epoch 814, Loss G:  66.8674, Loss D:  0.2846  g_loss mse comp   2.5665  0.1385\n",
            "Epoch 815, Loss G:  66.8763, Loss D:  0.2988  g_loss mse comp   2.5662  0.1560\n",
            "Epoch 816, Loss G:  67.6792, Loss D:  0.2819  g_loss mse comp   2.5972  0.1526\n",
            "Epoch 817, Loss G:  64.2624, Loss D:  0.2939  g_loss mse comp   2.4665  0.1344\n",
            "Epoch 818, Loss G:  65.7810, Loss D:  0.2961  g_loss mse comp   2.5245  0.1445\n",
            "Epoch 819, Loss G:  65.3307, Loss D:  0.3065  g_loss mse comp   2.5074  0.1372\n",
            "Epoch 820, Loss G:  65.4387, Loss D:  0.2839  g_loss mse comp   2.5113  0.1447\n",
            "Epoch 821, Loss G:  66.4306, Loss D:  0.2879  g_loss mse comp   2.5492  0.1519\n",
            "Epoch 822, Loss G:  66.0116, Loss D:  0.2954  g_loss mse comp   2.5329  0.1557\n",
            "Epoch 823, Loss G:  65.4146, Loss D:  0.2981  g_loss mse comp   2.5104  0.1439\n",
            "Epoch 824, Loss G:  65.6175, Loss D:  0.2751  g_loss mse comp   2.5181  0.1458\n",
            "Epoch 825, Loss G:  65.1424, Loss D:  0.2825  g_loss mse comp   2.5003  0.1337\n",
            "Epoch 826, Loss G:  65.4774, Loss D:  0.2827  g_loss mse comp   2.5128  0.1447\n",
            "Epoch 827, Loss G:  65.6405, Loss D:  0.2897  g_loss mse comp   2.5194  0.1368\n",
            "Epoch 828, Loss G:  66.0445, Loss D:  0.3011  g_loss mse comp   2.5348  0.1397\n",
            "Epoch 829, Loss G:  67.5816, Loss D:  0.2918  g_loss mse comp   2.5931  0.1604\n",
            "Epoch 830, Loss G:  64.8338, Loss D:  0.2854  g_loss mse comp   2.4881  0.1434\n",
            "Epoch 831, Loss G:  64.9607, Loss D:  0.2827  g_loss mse comp   2.4933  0.1351\n",
            "Epoch 832, Loss G:  65.9909, Loss D:  0.2800  g_loss mse comp   2.5327  0.1418\n",
            "Epoch 833, Loss G:  66.5144, Loss D:  0.2898  g_loss mse comp   2.5521  0.1588\n",
            "Epoch 834, Loss G:  66.3155, Loss D:  0.2824  g_loss mse comp   2.5448  0.1508\n",
            "Epoch 835, Loss G:  64.5236, Loss D:  0.2802  g_loss mse comp   2.4764  0.1384\n",
            "Epoch 836, Loss G:  65.5059, Loss D:  0.2901  g_loss mse comp   2.5137  0.1487\n",
            "Epoch 837, Loss G:  65.0893, Loss D:  0.2756  g_loss mse comp   2.4981  0.1390\n",
            "Epoch 838, Loss G:  64.5516, Loss D:  0.2700  g_loss mse comp   2.4776  0.1329\n",
            "Epoch 839, Loss G:  65.9121, Loss D:  0.2927  g_loss mse comp   2.5292  0.1522\n",
            "Epoch 840, Loss G:  66.3208, Loss D:  0.2947  g_loss mse comp   2.5453  0.1442\n",
            "Epoch 841, Loss G:  64.9690, Loss D:  0.2857  g_loss mse comp   2.4931  0.1476\n",
            "Epoch 842, Loss G:  64.7482, Loss D:  0.2858  g_loss mse comp   2.4851  0.1345\n",
            "Epoch 843, Loss G:  66.3734, Loss D:  0.2874  g_loss mse comp   2.5468  0.1575\n",
            "Epoch 844, Loss G:  65.2765, Loss D:  0.2872  g_loss mse comp   2.5055  0.1322\n",
            "Epoch 845, Loss G:  66.3907, Loss D:  0.2877  g_loss mse comp   2.5473  0.1609\n",
            "Epoch 846, Loss G:  64.5645, Loss D:  0.2780  g_loss mse comp   2.4781  0.1351\n",
            "Epoch 847, Loss G:  66.2802, Loss D:  0.2951  g_loss mse comp   2.5436  0.1459\n",
            "Epoch 848, Loss G:  66.1092, Loss D:  0.2901  g_loss mse comp   2.5369  0.1493\n",
            "Epoch 849, Loss G:  65.5595, Loss D:  0.2871  g_loss mse comp   2.5157  0.1517\n",
            "Epoch 850, Loss G:  64.9176, Loss D:  0.3017  g_loss mse comp   2.4912  0.1464\n",
            "Epoch 851, Loss G:  65.6689, Loss D:  0.2936  g_loss mse comp   2.5202  0.1439\n",
            "Epoch 852, Loss G:  66.7609, Loss D:  0.2743  g_loss mse comp   2.5616  0.1588\n",
            "Epoch 853, Loss G:  65.7982, Loss D:  0.2820  g_loss mse comp   2.5250  0.1494\n",
            "Epoch 854, Loss G:  67.5968, Loss D:  0.2733  g_loss mse comp   2.5934  0.1675\n",
            "Epoch 855, Loss G:  66.3389, Loss D:  0.2914  g_loss mse comp   2.5458  0.1486\n",
            "Epoch 856, Loss G:  65.4670, Loss D:  0.2827  g_loss mse comp   2.5126  0.1383\n",
            "Epoch 857, Loss G:  65.8759, Loss D:  0.3085  g_loss mse comp   2.5280  0.1485\n",
            "Epoch 858, Loss G:  66.7255, Loss D:  0.2771  g_loss mse comp   2.5607  0.1470\n",
            "Epoch 859, Loss G:  65.6483, Loss D:  0.2673  g_loss mse comp   2.5189  0.1582\n",
            "Epoch 860, Loss G:  65.7092, Loss D:  0.2786  g_loss mse comp   2.5213  0.1553\n",
            "Epoch 861, Loss G:  66.1308, Loss D:  0.2890  g_loss mse comp   2.5376  0.1520\n",
            "Epoch 862, Loss G:  66.8106, Loss D:  0.2813  g_loss mse comp   2.5638  0.1527\n",
            "Epoch 863, Loss G:  66.0567, Loss D:  0.3113  g_loss mse comp   2.5348  0.1511\n",
            "Epoch 864, Loss G:  65.4073, Loss D:  0.2923  g_loss mse comp   2.5103  0.1396\n",
            "Epoch 865, Loss G:  65.6956, Loss D:  0.2816  g_loss mse comp   2.5210  0.1502\n",
            "Epoch 866, Loss G:  67.1132, Loss D:  0.2765  g_loss mse comp   2.5751  0.1600\n",
            "Epoch 867, Loss G:  66.5369, Loss D:  0.2824  g_loss mse comp   2.5532  0.1550\n",
            "Epoch 868, Loss G:  66.6186, Loss D:  0.2967  g_loss mse comp   2.5560  0.1636\n",
            "Epoch 869, Loss G:  65.5665, Loss D:  0.2685  g_loss mse comp   2.5162  0.1450\n",
            "Epoch 870, Loss G:  67.2335, Loss D:  0.3129  g_loss mse comp   2.5801  0.1516\n",
            "Epoch 871, Loss G:  66.0381, Loss D:  0.3056  g_loss mse comp   2.5338  0.1588\n",
            "Epoch 872, Loss G:  66.7086, Loss D:  0.2849  g_loss mse comp   2.5594  0.1634\n",
            "Epoch 873, Loss G:  65.4936, Loss D:  0.2736  g_loss mse comp   2.5137  0.1386\n",
            "Epoch 874, Loss G:  65.0260, Loss D:  0.2919  g_loss mse comp   2.4955  0.1430\n",
            "Epoch 875, Loss G:  65.3732, Loss D:  0.2750  g_loss mse comp   2.5087  0.1475\n",
            "Epoch 876, Loss G:  66.3754, Loss D:  0.2837  g_loss mse comp   2.5471  0.1514\n",
            "Epoch 877, Loss G:  65.3084, Loss D:  0.2917  g_loss mse comp   2.5064  0.1424\n",
            "Epoch 878, Loss G:  66.4721, Loss D:  0.2854  g_loss mse comp   2.5507  0.1541\n",
            "Epoch 879, Loss G:  65.8336, Loss D:  0.2885  g_loss mse comp   2.5263  0.1511\n",
            "Epoch 880, Loss G:  66.1930, Loss D:  0.2733  g_loss mse comp   2.5401  0.1493\n",
            "Epoch 881, Loss G:  65.9429, Loss D:  0.2859  g_loss mse comp   2.5304  0.1524\n",
            "Epoch 882, Loss G:  66.3876, Loss D:  0.2748  g_loss mse comp   2.5476  0.1495\n",
            "Epoch 883, Loss G:  66.2192, Loss D:  0.3011  g_loss mse comp   2.5410  0.1530\n",
            "Epoch 884, Loss G:  65.3892, Loss D:  0.2962  g_loss mse comp   2.5092  0.1497\n",
            "Epoch 885, Loss G:  65.1640, Loss D:  0.2846  g_loss mse comp   2.5007  0.1464\n",
            "Epoch 886, Loss G:  65.7849, Loss D:  0.2935  g_loss mse comp   2.5243  0.1543\n",
            "Epoch 887, Loss G:  65.5854, Loss D:  0.2743  g_loss mse comp   2.5169  0.1450\n",
            "Epoch 888, Loss G:  66.6676, Loss D:  0.2617  g_loss mse comp   2.5580  0.1605\n",
            "Epoch 889, Loss G:  66.1150, Loss D:  0.2767  g_loss mse comp   2.5369  0.1568\n",
            "Epoch 890, Loss G:  65.6808, Loss D:  0.2767  g_loss mse comp   2.5204  0.1492\n",
            "Epoch 891, Loss G:  64.8859, Loss D:  0.2741  g_loss mse comp   2.4898  0.1508\n",
            "Epoch 892, Loss G:  66.6869, Loss D:  0.2842  g_loss mse comp   2.5587  0.1596\n",
            "Epoch 893, Loss G:  65.9279, Loss D:  0.2931  g_loss mse comp   2.5298  0.1533\n",
            "Epoch 894, Loss G:  65.7344, Loss D:  0.2754  g_loss mse comp   2.5222  0.1561\n",
            "Epoch 895, Loss G:  66.0017, Loss D:  0.2936  g_loss mse comp   2.5329  0.1466\n",
            "Epoch 896, Loss G:  65.7614, Loss D:  0.2728  g_loss mse comp   2.5235  0.1502\n",
            "Epoch 897, Loss G:  65.2422, Loss D:  0.2860  g_loss mse comp   2.5037  0.1456\n",
            "Epoch 898, Loss G:  67.6351, Loss D:  0.2753  g_loss mse comp   2.5950  0.1654\n",
            "Epoch 899, Loss G:  67.0610, Loss D:  0.2867  g_loss mse comp   2.5731  0.1614\n",
            "Epoch 900, Loss G:  66.1918, Loss D:  0.2835  g_loss mse comp   2.5394  0.1663\n",
            "Epoch 901, Loss G:  65.0691, Loss D:  0.2923  g_loss mse comp   2.4966  0.1565\n",
            "Epoch 902, Loss G:  64.8356, Loss D:  0.2843  g_loss mse comp   2.4881  0.1438\n",
            "Epoch 903, Loss G:  65.9550, Loss D:  0.2769  g_loss mse comp   2.5311  0.1453\n",
            "Epoch 904, Loss G:  65.6760, Loss D:  0.2856  g_loss mse comp   2.5200  0.1548\n",
            "Epoch 905, Loss G:  66.3723, Loss D:  0.2665  g_loss mse comp   2.5467  0.1588\n",
            "Epoch 906, Loss G:  64.2321, Loss D:  0.2781  g_loss mse comp   2.4651  0.1400\n",
            "Epoch 907, Loss G:  65.8538, Loss D:  0.2804  g_loss mse comp   2.5269  0.1556\n",
            "Epoch 908, Loss G:  66.2556, Loss D:  0.2920  g_loss mse comp   2.5423  0.1549\n",
            "Epoch 909, Loss G:  65.2285, Loss D:  0.2881  g_loss mse comp   2.5030  0.1515\n",
            "Epoch 910, Loss G:  65.7314, Loss D:  0.2727  g_loss mse comp   2.5224  0.1484\n",
            "Epoch 911, Loss G:  66.1883, Loss D:  0.2761  g_loss mse comp   2.5397  0.1555\n",
            "Epoch 912, Loss G:  65.8160, Loss D:  0.2816  g_loss mse comp   2.5251  0.1628\n",
            "Epoch 913, Loss G:  66.5835, Loss D:  0.3114  g_loss mse comp   2.5546  0.1652\n",
            "Epoch 914, Loss G:  66.1507, Loss D:  0.2773  g_loss mse comp   2.5382  0.1580\n",
            "Epoch 915, Loss G:  66.3100, Loss D:  0.2932  g_loss mse comp   2.5440  0.1660\n",
            "Epoch 916, Loss G:  65.7227, Loss D:  0.2705  g_loss mse comp   2.5220  0.1513\n",
            "Epoch 917, Loss G:  65.9705, Loss D:  0.2876  g_loss mse comp   2.5312  0.1599\n",
            "Epoch 918, Loss G:  66.0346, Loss D:  0.2702  g_loss mse comp   2.5338  0.1549\n",
            "Epoch 919, Loss G:  65.5006, Loss D:  0.2695  g_loss mse comp   2.5135  0.1501\n",
            "Epoch 920, Loss G:  65.6612, Loss D:  0.2895  g_loss mse comp   2.5195  0.1554\n",
            "Epoch 921, Loss G:  65.2228, Loss D:  0.2776  g_loss mse comp   2.5030  0.1444\n",
            "Epoch 922, Loss G:  65.6668, Loss D:  0.2833  g_loss mse comp   2.5197  0.1539\n",
            "Epoch 923, Loss G:  66.6952, Loss D:  0.2765  g_loss mse comp   2.5591  0.1594\n",
            "Epoch 924, Loss G:  65.3139, Loss D:  0.2851  g_loss mse comp   2.5063  0.1504\n",
            "Epoch 925, Loss G:  66.4461, Loss D:  0.2851  g_loss mse comp   2.5492  0.1664\n",
            "Epoch 926, Loss G:  66.6326, Loss D:  0.2768  g_loss mse comp   2.5560  0.1762\n",
            "Epoch 927, Loss G:  66.4559, Loss D:  0.2919  g_loss mse comp   2.5496  0.1669\n",
            "Epoch 928, Loss G:  65.8291, Loss D:  0.2784  g_loss mse comp   2.5260  0.1543\n",
            "Epoch 929, Loss G:  65.5038, Loss D:  0.2706  g_loss mse comp   2.5132  0.1615\n",
            "Epoch 930, Loss G:  66.5900, Loss D:  0.2557  g_loss mse comp   2.5550  0.1600\n",
            "Epoch 931, Loss G:  66.2172, Loss D:  0.2701  g_loss mse comp   2.5411  0.1485\n",
            "Epoch 932, Loss G:  65.5404, Loss D:  0.2916  g_loss mse comp   2.5150  0.1513\n",
            "Epoch 933, Loss G:  66.9380, Loss D:  0.2869  g_loss mse comp   2.5682  0.1651\n",
            "Epoch 934, Loss G:  66.7238, Loss D:  0.2663  g_loss mse comp   2.5600  0.1645\n",
            "Epoch 935, Loss G:  64.8846, Loss D:  0.2942  g_loss mse comp   2.4900  0.1446\n",
            "Epoch 936, Loss G:  65.0716, Loss D:  0.2808  g_loss mse comp   2.4969  0.1510\n",
            "Epoch 937, Loss G:  66.6304, Loss D:  0.2662  g_loss mse comp   2.5563  0.1660\n",
            "Epoch 938, Loss G:  65.6388, Loss D:  0.2762  g_loss mse comp   2.5186  0.1543\n",
            "Epoch 939, Loss G:  66.0949, Loss D:  0.2876  g_loss mse comp   2.5357  0.1655\n",
            "Epoch 940, Loss G:  66.0328, Loss D:  0.2716  g_loss mse comp   2.5334  0.1650\n",
            "Epoch 941, Loss G:  65.7290, Loss D:  0.2763  g_loss mse comp   2.5219  0.1590\n",
            "Epoch 942, Loss G:  65.6477, Loss D:  0.2691  g_loss mse comp   2.5190  0.1533\n",
            "Epoch 943, Loss G:  65.0509, Loss D:  0.2815  g_loss mse comp   2.4960  0.1559\n",
            "Epoch 944, Loss G:  64.8468, Loss D:  0.2874  g_loss mse comp   2.4881  0.1562\n",
            "Epoch 945, Loss G:  66.2698, Loss D:  0.2722  g_loss mse comp   2.5428  0.1564\n",
            "Epoch 946, Loss G:  64.8920, Loss D:  0.2744  g_loss mse comp   2.4903  0.1443\n",
            "Epoch 947, Loss G:  65.9057, Loss D:  0.2839  g_loss mse comp   2.5288  0.1564\n",
            "Epoch 948, Loss G:  65.4586, Loss D:  0.2789  g_loss mse comp   2.5119  0.1500\n",
            "Epoch 949, Loss G:  65.3311, Loss D:  0.2836  g_loss mse comp   2.5065  0.1619\n",
            "Epoch 950, Loss G:  66.8023, Loss D:  0.2616  g_loss mse comp   2.5633  0.1572\n",
            "Epoch 951, Loss G:  65.3323, Loss D:  0.2620  g_loss mse comp   2.5063  0.1690\n",
            "Epoch 952, Loss G:  66.1514, Loss D:  0.2992  g_loss mse comp   2.5384  0.1541\n",
            "Epoch 953, Loss G:  66.5300, Loss D:  0.2878  g_loss mse comp   2.5524  0.1671\n",
            "Epoch 954, Loss G:  66.8996, Loss D:  0.2665  g_loss mse comp   2.5667  0.1651\n",
            "Epoch 955, Loss G:  65.7094, Loss D:  0.2871  g_loss mse comp   2.5213  0.1565\n",
            "Epoch 956, Loss G:  66.3987, Loss D:  0.2715  g_loss mse comp   2.5475  0.1627\n",
            "Epoch 957, Loss G:  66.0909, Loss D:  0.2767  g_loss mse comp   2.5359  0.1586\n",
            "Epoch 958, Loss G:  65.3769, Loss D:  0.2820  g_loss mse comp   2.5086  0.1524\n",
            "Epoch 959, Loss G:  66.0518, Loss D:  0.2955  g_loss mse comp   2.5341  0.1660\n",
            "Epoch 960, Loss G:  66.5081, Loss D:  0.2780  g_loss mse comp   2.5515  0.1686\n",
            "Epoch 961, Loss G:  66.5441, Loss D:  0.2666  g_loss mse comp   2.5531  0.1631\n",
            "Epoch 962, Loss G:  64.7314, Loss D:  0.2778  g_loss mse comp   2.4845  0.1336\n",
            "Epoch 963, Loss G:  66.1965, Loss D:  0.2894  g_loss mse comp   2.5401  0.1531\n",
            "Epoch 964, Loss G:  66.1507, Loss D:  0.2882  g_loss mse comp   2.5383  0.1552\n",
            "Epoch 965, Loss G:  64.9738, Loss D:  0.2726  g_loss mse comp   2.4933  0.1471\n",
            "Epoch 966, Loss G:  64.5017, Loss D:  0.2680  g_loss mse comp   2.4750  0.1518\n",
            "Epoch 967, Loss G:  65.4403, Loss D:  0.3032  g_loss mse comp   2.5112  0.1483\n",
            "Epoch 968, Loss G:  65.3324, Loss D:  0.2683  g_loss mse comp   2.5071  0.1487\n",
            "Epoch 969, Loss G:  65.5459, Loss D:  0.2866  g_loss mse comp   2.5148  0.1607\n",
            "Epoch 970, Loss G:  65.2274, Loss D:  0.2822  g_loss mse comp   2.5028  0.1537\n",
            "Epoch 971, Loss G:  65.7517, Loss D:  0.2922  g_loss mse comp   2.5226  0.1631\n",
            "Epoch 972, Loss G:  65.5747, Loss D:  0.2656  g_loss mse comp   2.5159  0.1602\n",
            "Epoch 973, Loss G:  66.4319, Loss D:  0.2853  g_loss mse comp   2.5489  0.1615\n",
            "Epoch 974, Loss G:  65.4678, Loss D:  0.2864  g_loss mse comp   2.5121  0.1527\n",
            "Epoch 975, Loss G:  66.7580, Loss D:  0.2918  g_loss mse comp   2.5610  0.1729\n",
            "Epoch 976, Loss G:  66.1205, Loss D:  0.2726  g_loss mse comp   2.5369  0.1599\n",
            "Epoch 977, Loss G:  66.9240, Loss D:  0.2808  g_loss mse comp   2.5676  0.1665\n",
            "Epoch 978, Loss G:  64.3343, Loss D:  0.2769  g_loss mse comp   2.4686  0.1516\n",
            "Epoch 979, Loss G:  66.4793, Loss D:  0.2642  g_loss mse comp   2.5505  0.1661\n",
            "Epoch 980, Loss G:  66.6250, Loss D:  0.2870  g_loss mse comp   2.5560  0.1688\n",
            "Epoch 981, Loss G:  64.2098, Loss D:  0.2882  g_loss mse comp   2.4638  0.1497\n",
            "Epoch 982, Loss G:  66.0872, Loss D:  0.2600  g_loss mse comp   2.5359  0.1550\n",
            "Epoch 983, Loss G:  65.6123, Loss D:  0.2716  g_loss mse comp   2.5176  0.1541\n",
            "Epoch 984, Loss G:  65.3611, Loss D:  0.2886  g_loss mse comp   2.5075  0.1667\n",
            "Epoch 985, Loss G:  67.9450, Loss D:  0.2922  g_loss mse comp   2.6065  0.1755\n",
            "Epoch 986, Loss G:  66.2014, Loss D:  0.2657  g_loss mse comp   2.5399  0.1649\n",
            "Epoch 987, Loss G:  65.8645, Loss D:  0.2646  g_loss mse comp   2.5271  0.1608\n",
            "Epoch 988, Loss G:  66.0017, Loss D:  0.2782  g_loss mse comp   2.5325  0.1559\n",
            "Epoch 989, Loss G:  65.1072, Loss D:  0.2629  g_loss mse comp   2.4981  0.1575\n",
            "Epoch 990, Loss G:  66.1125, Loss D:  0.2822  g_loss mse comp   2.5363  0.1699\n",
            "Epoch 991, Loss G:  65.1883, Loss D:  0.2935  g_loss mse comp   2.5008  0.1676\n",
            "Epoch 992, Loss G:  65.7191, Loss D:  0.2780  g_loss mse comp   2.5214  0.1627\n",
            "Epoch 993, Loss G:  65.4960, Loss D:  0.2898  g_loss mse comp   2.5130  0.1573\n",
            "Epoch 994, Loss G:  65.3465, Loss D:  0.2808  g_loss mse comp   2.5073  0.1554\n",
            "Epoch 995, Loss G:  65.5378, Loss D:  0.2762  g_loss mse comp   2.5148  0.1520\n",
            "Epoch 996, Loss G:  67.3651, Loss D:  0.2534  g_loss mse comp   2.5844  0.1714\n",
            "Epoch 997, Loss G:  65.2444, Loss D:  0.2921  g_loss mse comp   2.5037  0.1479\n",
            "Epoch 998, Loss G:  66.5860, Loss D:  0.2810  g_loss mse comp   2.5549  0.1584\n",
            "Epoch 999, Loss G:  65.9347, Loss D:  0.2683  g_loss mse comp   2.5297  0.1614\n"
          ]
        }
      ]
    }
  ]
}